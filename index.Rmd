---
title: "Opioids in United States"
css: style.css
output:
  html_document:
    self_contained: yes
    code_download: yes
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes

---

<style>
#TOC {
  background: url("https://opencasestudies.github.io/img/logo.jpg");
  background-size: contain;
  padding-top: 240px !important;
  background-repeat: no-repeat;
}
</style>


<!-- Open all links in new tab-->  
<base target="_blank"/> 

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, comment = NA, echo = TRUE,
                      message = FALSE, warning = FALSE, cache = FALSE,
                      fig.align = "center", out.width = '90%')
library(here)
library(knitr)
library(readr)
```

#### {.outline }
```{r, echo=FALSE}
knitr::include_graphics(here("img",
                             "mainplot.png"))
```

####

AVOCADO: the title of the figure feels somewhat complicated and should be simplified. Might comment below in the last section after reading through the entire case study though before making suggestions. 

#### {.disclaimer_block}

**Disclaimer**: The purpose of the [Open Case Studies](https://opencasestudies.github.io){target="_blank"} project is **to demonstrate the use of various data science methods, tools, and software in the context of messy, real-world data**. A given case study does not cover all aspects of the research process, is not claiming to be the most appropriate way to analyze a given data set, and should not be used in the context of making policy decisions without external consultation from scientific experts. 

####

#### {.license_block}

This work is licensed under the Creative Commons Attribution-NonCommercial 3.0 [(CC BY-NC 3.0)](https://creativecommons.org/licenses/by-nc/3.0/us/){target="_blank"} United States License.

####

#### {.reference_block}

To cite this case study please use:

Wright, Carrie, and Wang, Kexin and Jager, Leah and Taub, Margaret and Hicks, Stephanie. (2020). https://opencasestudies.github.io/ocs-bp-opioid-rural-urban/ Opioids in the United States (Version v1.0.0).

avocado update url
####

# **Motivation**
*** 


In this case study we will be examining the number of opioid pills (specifically [oxycodone](https://en.wikipedia.org/wiki/Opioid_epidemic_in_the_United_States) and [hydrocodone](https://en.wikipedia.org/wiki/Opioid_epidemic_in_the_United_States), as they are the top two most abused opioids) shipped to pharmacies and practitioners at the county-level around the United States (US) from 2006 to 2014.

This data comes from the [DEA](https://www.dea.gov/) [Automated Reports and Consolidated Ordering System (ARCOS)](https://www.deadiversion.usdoj.gov/arcos/retail_drug_summary/) and was released by the [Washington Post](https://www.washingtonpost.com/) after legal action by the owner of the [Charleston Gazette-Mail](https://www.wvgazettemail.com/) in West Virginia and the [Washington Post](https://www.washingtonpost.com/).

We will investigate how the number of shipped pills has changed across time and between rural and urban counties in the US. This analysis will demonstrate how different regions of the country may have been more at risk for opioid addiction crises due to differing rates of opioid prescription (using the number of pills as a proxy for prescription rates). This will help inform students about how evidence-based intervention decisions are made in this area.  

This case study is motivated by this [article](https://www.cdc.gov/mmwr/volumes/68/wr/mm6802a1.htm?s_cid=mm6802a1_w):

#### {.reference_block}

García, M. C. et al. Opioid Prescribing Rates in Nonmetropolitan and Metropolitan Counties Among Primary Care Providers Using an Electronic Health Record System — United States, 2014–2017. MMWR Morb. Mortal. Wkly. Rep. 68, 25–30 (2019). DOI: [10.15585/mmwr.mm6802a1](http://dx.doi.org/10.15585/mmwr.mm6802a1)

####

This article explores rates of opioid prescriptions in rural and urban communities in the United States using the [Athenahealth electronic health record (EHR) system](https://landing.athenahealth.com/g/improvecare?cmp=10672941) for 31,422 primary care providers from January 2014 to March 2017.

The main takeaways from this article were:

> Among 70,237 fatal drug overdoses in 2017, prescription opioids were involved in 17,029 (24.2%).

> The percentage of patients prescribed an opioid was higher in **rural** than in urban areas. 

> Higher opioid prescribing rates put patients **at risk for addiction and overdose**.

Indeed, this was confirmed by another [article](https://jamanetwork.com/journals/jamapsychiatry/fullarticle/1874575), which surveyed heroin users in the [Survey of Key Informants’ Patients Program](https://www.radars.org/radars-system-programs/survey-of-key-informants-patients.html) and the [Researchers and Participants Interacting Directly (RAPID) program](https://www.radars.org/radars-system-programs/researchers-and-participants-interacting-directly.html).

#### {.reference_block}

Cicero, T. J., Ellis, M. S., Surratt, H. L. & Kurtz, S. P. The Changing Face of Heroin Use in the United States: A Retrospective Analysis of the Past 50 Years. JAMA Psychiatry 71, 821 (2014). [DOI:10.1001/jamapsychiatry.2014.366](https://doi.org/10.1001/jamapsychiatry.2014.366)

####

They found that:

> Respondents who began using heroin in the 1960s were predominantly young men (82.8%; mean age, 16.5 years) whose first opioid of abuse was heroin (80%).

AVOCADO: what does the last 80% refer to? 80% heroin and 20% not? I think so based on the figure below.

> However, more **recent users** were older (mean age, 22.9 years) men and women **living in less urban areas (75.2%)** who were **introduced to opioids through prescription drugs (75.0%)**.

```{r, out.width = "80%", echo = FALSE, fig.align ="center"}
include_graphics(here::here("img", "introducedbyprescription.png"))
```

##### [[sourcce]]((https://doi.org/10.1001/jamapsychiatry.2014.366))

> Heroin use has **changed from an inner-city**, minority-centered problem to one that has a more widespread geographical distribution, involving primarily white men and women in their late 20s living **outside of large urban areas**.

```{r, out.width = "50%", echo = FALSE, fig.align ="center"}
include_graphics(here::here("img", "james-yarema-5tyMgag0wRo-unsplash.jpg"))
```

<span>Photo by <a href="https://unsplash.com/@jamesyarema?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">James Yarema</a> on <a href="https://unsplash.com/s/photos/pills?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></span>

> A much greater percentage of heroin users completing the survey in the SKIP Program reported currently living in **small urban or nonurban areas** than in large urban areas (75.2% vs 24.8%) at the time of survey completion. 

This survey used self-declared area of current residence (large urban, small urban, suburban, or rural).


# **Main Question**
*** 

#### {.main_question_block}
<b><u> Our main question: </u></b>

How did opioid shipment rates differ between rural and urban regions over time in the US from 2006-2014?


####

# **Learning Objectives** 
*** 

In this case study, we will demonstrate how to obtain data from an [Application Programming Interface (API)](https://en.wikipedia.org/wiki/API), which is an interface that allows users to more easily interact with a database. We will also especially focus on using packages and functions from the [`Tidyverse`](https://www.tidyverse.org/){target="_blank"}, such as `dplyr`, `tidyr`. The tidyverse is a library of packages created by RStudio. While some students may be familiar with previous R programming packages, these packages make data science in R more legible and intuitive.


```{r, out.width = "20%", echo = FALSE, fig.align ="center"}
include_graphics("https://tidyverse.tidyverse.org/logo.png")
```

The skills, methods, and concepts that students will be familiar with by the end of this case study are:

Data science skills:  
  
1. Importing data from an [API](https://en.wikipedia.org/wiki/API) (`httr` and `jasonlite`)  
2. How to join data with `dplyr`
3. How to reshape data by pivoting between "long" and "wide" formats and drop rows with `NA` values (`tidyr`)  
4. How to create formatted tables of data with `formattable`  
5. How to look for missing data in a dataset (`naniar`)  
6. How to create data visualizations with `ggplot2` 
7. How to create interactive plots that are difficult to label because they have  many elements (`ggiraph`)  
8. How to combine individual plots into one figure with `patchwork`

  
Statistical concepts and methods:  
  
1. Understanding of when and why data normalization is useful  
2. Understanding of how group definitions can change results  
3. Understanding of when to use a Wilcoxon rank sum test (also called Mann Whitney U test)  
4. How to implement a Wilcoxon rank sum test in R  
5. How to interpret a Wilcoxon rank sum test    

*** 


We will begin by loading the packages that we will need:

```{r}
library(readxl)
library(tibble)
library(httr)
library(jsonlite)
library(stringr)
library(magrittr)
library(dplyr)
library(tidyr)
library(naniar)
library(ggplot2)
library(formattable)
library(forcats)
library(ggpol)
library(ggiraph)
library(patchwork)
library(directlabels)
library(usdata)
```



 <u>**Packages used in this case study:** </u>

Package   | Use in this case study                                                                      
---------- |-------------
[readxl](https://readxl.tidyverse.org/index.html) | to import an excel file   
[httr](https://httr.r-lib.org/) | to retrieve data from an API   
[tibble](https://tibble.tidyverse.org/) | to create tibbles (the tidyverse version of dataframes)   
[jsonlite](https://cran.r-project.org/web/packages/jsonlite/jsonlite.pdf) | to parse json files   
[stringr](https://stringr.tidyverse.org/){target="_blank"}      | to manipulate  character strings within the data (subset and detect parts of strings)    
[dplyr](https://dplyr.tidyverse.org/){target="_blank"}      | to filter, subset, join, and modify and summarize the data   
[magrittr](https://magrittr.tidyverse.org/){target="_blank"}      | to pipe sequential commands   
[tidyr](https://tidyr.tidyverse.org/){target="_blank"}      | to change the shape or format of tibbles to wide and long   
[naniar](https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html) | to get a sense of missing data   
[ggplot2](https://ggplot2.tidyverse.org/){target="_blank"}      | to create plots
[formattable](https://cran.r-project.org/web/packages/formattable/formattable.pdf) | to create a formatted table
[forcats](https://forcats.tidyverse.org/){target="_blank"}      | to reorder factor for plot
[ggpol](https://cran.r-project.org/web/packages/ggpol/ggpol.pdf) | to create plots that are have jitter and half boxplots   
[ggiraph](https://cran.r-project.org/web/packages/ggiraph/ggiraph.pdf)   | to create interactive plots
[patchwork](https://cran.r-project.org/web/packages/patchwork/patchwork.pdf) | to combine plots
[directlabels](https://cran.r-project.org/web/packages/directlabels/directlabels.pdf) | to add labels directly on lines within plots
[usdata]() | AVOCADO: needs to be added

The first time we use a function, we will use the `::` to indicate which package we are using. Unless we have overlapping function names, this is not necessary, but we will include it here to be informative about where the functions we will use come from.


# **Context**
*** 
**What exactly are opioids?**

According to the [DEA](https://www.dea.gov/taxonomy/term/331) and the [Alta Mira addiction treatment center](https://www.altamirarecovery.com/opiates/difference-opiates-opioids/):

Opioids (also known as narcotics which comes from the Greek word for "stupor"), describes a class of drugs that contain [opium](https://en.wikipedia.org/wiki/Opium) (the poppy plant - [*Papaver somniferum*](https://en.wikipedia.org/wiki/Papaver_somniferum)), are derived from opium, or contain a semi-synthetic or synthetic substitute for opium.

```{r, echo = FALSE}
knitr::include_graphics(here::here("img","ingo-doerrie-Ti6Sk5rZRP8-unsplash.jpg" ))

```

<span>Photo by <a href="https://unsplash.com/@ingodoerrie?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Ingo Doerrie</a> on <a href="https://unsplash.com/s/photos/opium?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></span>


However, technically, opioids are substances that bind to the [opioid receptors](https://www.sciencedaily.com/releases/2007/10/071014163647.htm#:~:text=The%20opioid%20system%20consists%20of,and%20potentially%20initiating%20addictive%20behaviors.) in the body, which are involved in the sensation of `pain` and the experience of [reward](https://en.wikipedia.org/wiki/Reward_system#:~:text=The%20reward%20system%20is%20a,involve%20pleasure%20as%20a%20core). There are actually opioids that naturally are made by the human body, the most well known being the [endorphins](https://www.medicalnewstoday.com/articles/320839#:~:text=Endorphins%20are%20chemicals%20produced%20by,surgery%20or%20for%20pain%2Drelief.).

***

<details> <summary> Click here to learn about why opioids are addictive </summary>

Opioid drugs tend to be addictive because they modulate the [reward system](https://en.wikipedia.org/wiki/Reward_system#:~:text=The%20reward%20system%20is%20a,involve%20pleasure%20as%20a%20core). This is the part of the brain that reinforces behaviors (normally these are behaviors such as drinking water or eating food) by causing the experience of pleasure (through the release of a neurotransmitter called [dopamine](https://en.wikipedia.org/wiki/Dopamine)). 

This same system can be activated by both opioids that naturally occur in the body, as well as opioid prescription drugs and other addictive drugs. Activation of this system by drug use  leads to very high releases of Dopamine and the sensation of pleasure which ultimately leads to reinforced use of that drug.

```{r, echo = FALSE}
knitr::include_graphics("https://www.drugabuse.gov/sites/default/files/styles/content_image_large/public/drugstargetthebrainspleasurecenter.gif?itok=Ffd_PCeb")
```

[[source](https://www.drugabuse.gov/publications/drugs-brains-behavior-science-addiction/drugs-brain)]

</details> 

***

In general, opioid medications and drugs have been found to dull the senses, relieve pain, supress cough, reduce respiration and heart rate, induce constipation, and induce feelings of euphoria. They have a high potential for abuse and addiction.

Drugs within this class include (with prescription drug brand names are shown in parentheses): 

1) Non-synthetic purified: Morphine, Codeine, Thebaine
2) Semi-synthetic:  Heroin, Oxycodone (OxyContin, Oxecta, Roxicodone), and Hydrocodone ( Vicodin, Lortab, Lorcet)), oxymorphone (Opana), Hydromorphone (Dilaudid, Exalgo)
3) Synthetic: Meperidine (Demerol), Methadone (Methadose, Dolophine), and Fentanyl (Abstral, Actiq, Fentora, Duragesic, Lazanda, Subsys), Tramadol (ConZip, Ryzolt, Ultram)


```{r, echo = FALSE, out.width="40%"}
knitr::include_graphics(here::here("img","Opium_pod_cut_to_demonstrate_fluid_extraction1.jpg" ))

```

##### [[source]](https://en.wikipedia.org/wiki/File:Opium_pod_cut_to_demonstrate_fluid_extraction1.jpg)

Opium comes from the fluid (which is also called poppy tears) inside the seed capsules of the [*Papaver somniferum*](https://en.wikipedia.org/wiki/Papaver_somniferum) plant. This contains morphine, codeine, and thebaine. This is then dried. 

Opium has been used by humans since 5000 BCE and it has been used across the world. See [here](https://en.wikipedia.org/wiki/Opium) for an interesting overview of the history. 

***
<details> <summary> Click here to learn about the history of opioids in the US </summary>

Opium derived medications were historically used in United States to treat a variety of ailments besides pain including: cholera, dysentery, tuberculosis, and mental illness.  

Of note, they state that "from 1898 to 1910 heroin was marketed as a non-addictive morphine substitute and cough medicine for children"!

Here you can see a photo of a bottle of heroin:

```{r, echo = FALSE, out.width="40%"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Bayer_Heroin_bottle.jpg/220px-Bayer_Heroin_bottle.jpg")
```

[[source](https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Bayer_Heroin_bottle.jpg/220px-Bayer_Heroin_bottle.jpg)]

Opioids have continued to be used in the treatment of pain. 

</details> 

***

## The Opioid Epidemic 

The opioid epidemic began in the late 1990s. 

According to the [US department of health and human services (HHS)](
https://www.hhs.gov/opioids/about-the-epidemic/index.html):

> In the late 1990s, pharmaceutical companies reassured the medical community that patients would not become addicted to opioid pain relievers and healthcare providers began to prescribe them at greater rates.

> Increased prescription of opioid medications led to **widespread misuse** of both prescription and non-prescription opioids before it became clear that these medications could indeed be highly addictive.

> In 2017 the [HHS](https://www.hhs.gov) declared a public health emergency 

See [here](https://en.wikipedia.org/wiki/Timeline_of_the_opioid_epidemic) for a timeline of the epidemic in the US and [here](https://en.wikipedia.org/wiki/Opioid_epidemic_in_the_United_States) for more details about the epidemic.


According to this [article](https://www.cdc.gov/mmwr/volumes/68/wr/mm6802a1.htm?s_cid=mm6802a1_w) from the [Morbidity and Mortality Weekly Report (MMWR)](https://www.cdc.gov/mmwr/about.html) of the [Centers for Disease Control and Prevention (CDC)](https://en.wikipedia.org/wiki/Centers_for_Disease_Control_and_Prevention):

> Drug overdose is the **leading cause** of unintentional injury-associated death in the United States.

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "Opioids_Infographic.png"))
```

##### [[source]](https://www.hhs.gov/opioids/sites/default/files/2019-11/Opioids%20Infographic_letterSizePDF_10-02-19.pdf)


According to the [CDC](https://www.cdc.gov/drugoverdose/epidemic/index.html), there were 3 waves of the epidemic:

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "2018-3-Wave-Lines-Mortality.png"))

```

#### [[source]](https://www.cdc.gov/drugoverdose/images/epidemic/2018-3-Wave-Lines-Mortality.png)

You can see that most recent overdose deaths were due to the use of **synthetic opioids**, where as previous high levels of overdoses (till about 2015) were attributable to natural and semi-synthetic opioids (which is what we will look at in this case study).

They state that: 

> From 1999–2018, almost **450,000** people died from an **overdose involving any opioid**, including prescription and illicit opioids.


Importantly rates appear to differ across states, according to this [CDC report](https://www.cdc.gov/mmwr/volumes/67/wr/mm675152e1.htm?s_cid=mm675152e1_w) 


```{r, echo = FALSE}
knitr::include_graphics("https://www.cdc.gov/mmwr/volumes/67/wr/figures/mm675152e1-F.gif")

```

##### [[source]](https://www.cdc.gov/mmwr/volumes/67/wr/figures/mm675152e1-F.gif)


According to the [motivating report](https://www.cdc.gov/mmwr/volumes/68/wr/pdfs/mm6802a1-H.pdf) for our case study: 

Prescription rates are now declining, however, prescription of opioids was found to be higher in rural areas rather than urban ares. 

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "context.png"))

```

##### [[source]](https://www.cdc.gov/mmwr/volumes/68/wr/pdfs/mm6802a1-H.pdf)
 
It is important to identify locations where people are particularly vulnerable to target interventions for communities that need it the most.

 
# **Limitations**
*** 

There are some important considerations regarding this data analysis to keep in mind: 

According to the [Washington Post database](https://www.washingtonpost.com/national/2019/07/18/how-download-use-dea-pain-pills-database/), they state about the DEA data:

>"It’s important to remember that the number of pills in each county does not necessarily mean those pills went to people who live in that county. The data only shows us what pharmacies the pills are shipped to and nothing else."

Furthermore, we will define counties as being rural or urban however there can be great variation within a county and we used land area values from only 2010 even though these can fluctuate. Therefore, the way we categorized counties should be seen as an approximation. Additionally, other aspects about a county besides population level and density can be influential for creating an environment that would increase the vulnerability of community members to drug abuse and addiction. These include socioeconomic factors among others. 

Finally, overdose deaths are often due to the use of multiple substances. Simply because a county received more pills does notmean that people in that county would experience more drug overdoses. It is also important to remember that prescription opioids only account for a portion of the drug overdose deaths reported in this time period. However, according to this [article](https://jamanetwork.com/journals/jamapsychiatry/fullarticle/1874575), 75% of heroin users surveyed were introduced to opioids through prescription drug use.

The recent overdose deaths in the US opioid crisis are mostly due to synthetic opioids. Understanding what makes communities vulnerable to these types of opioids is an area of active research but not a focus of this case study.

# **What are the data?**
*** 

We will use data from two sources:

1. The US census for land area of counties to allow us to estimate county-level population density  

2. The [Washington Post data](https://www.washingtonpost.com/national/2019/07/18/how-download-use-dea-pain-pills-database/) from the [Drug Enforcement Administration (DEA)](https://www.dea.gov/) about opioid ([oxycodone](https://www.dea.gov/sites/default/files/2020-06/Oxycodone-2020_0.pdf) and [hydrocodone](https://www.deadiversion.usdoj.gov/drug_chem_info/hydrocodone.pdf)) pill shipments to pharmacies and practitioners around the US at the county-level

This second dataset was released in July of 2019 and has been controversial as according to the Washington Post:

> The disclosure is part of a civil action brought by 2,500 cities, towns, counties and tribal nations alleging that nearly two dozen drug companies **conspired to saturate the nation with opioids**.

See [here](https://www.washingtonpost.com/national/2019/07/20/opioid-files/?arc404=true) for more details about how this database was released.

The [Washington Post](https://www.washingtonpost.com/national/2019/07/18/how-download-use-dea-pain-pills-database/) states that they:

>.. cleaned the data to include only information on shipments of oxycodone and hydrocodone pills. We did not include data on 10 other opioids because they were shipped in much lower quantities...

>It’s important to remember that the number of pills in each county does not necessarily mean those pills went to people who live in that county. The data only shows us what pharmacies the pills are shipped to and nothing else.

AVOCADO: the paragraph was repeated above in the limitations? Should we only include it either there or here? 

This data was part of the [Automated Reports and Consolidated Ordering System (ARCOS)](https://www.deadiversion.usdoj.gov/arcos/retail_drug_summary/) of the DEA in which:

> manufacturers and distributors report their controlled substances transactions

Their [website](https://www.deadiversion.usdoj.gov/arcos/index.html#background) indicates that: 

> The Controlled Substances Act of 1970 created the requirement for Manufacturers and Distributors to report their controlled substances transactions to the Attorney General. The Attorney General delegates this authority to the Drug Enforcement Administration (DEA).

> ARCOS is an automated, comprehensive drug reporting system which monitors the flow of DEA controlled substances from their point of manufacture through commercial distribution channels to point of sale or distribution at the dispensing/retail level - hospitals, retail pharmacies, practitioners, mid-level practitioners, and teaching institutions. Included in the list of controlled substance transactions tracked by ARCOS are the following: All Schedules I and II materials (manufacturers and distributors); Schedule III narcotic and gamma-hydroxybutyric acid (GHB) materials (manufacturers and distributors); and selected Schedule III and IV psychotropic drugs (manufacturers only).

The annual report about the data from 2019, can be found [here](https://www.deadiversion.usdoj.gov/arcos/retail_drug_summary/report_yr_2019.pdf).

As this is a very large dataset, thus the Washington Post created an [application prgoramming interface (API)](https://en.wikipedia.org/wiki/API)  to make it easier for users to access the data. 

An API is a computational interface that simplifies interactions with a data or file system for a user. It is similar to a [Graphical User Interface (GUI)](https://en.wikipedia.org/wiki/Graphical_user_interface), yet it allows the user some more flexibility/functionality.

This [link](https://arcos-api.ext.nile.works/__swagger__/) takes you to the Washington Post ARCOS API. 

There was also an R package on cran called [arcos](https://cran.r-project.org/package=arcos) for interacting with the API, but this has been archived.  This package is however still available [here](https://github.com/wpinvestigative/arcos) on Github.

See [here](https://www.washingtonpost.com/national/2019/07/18/how-download-use-dea-pain-pills-database/) for more information about how to get access the Washington Post DEA database.


# **Data Import**
*** 

## Land Area

We will need land area data for our calculations of population density. 

We obtained county land area data from the US census Bureau at this [link](https://www.census.gov/library/publications/2011/compendia/usa-counties-2011.html#LND).

This [link](https://www.census.gov/library/publications/2011/compendia/usa-counties-2011/file-layout.html) explains how the data is formatted.

We will use the `read_excel()` function of the `readxl` package to import the data. We will also convert the data into a [tibble](https://tibble.tidyverse.org/) (which is a the tidyverse version of a data frame) by using the `as_tibble()` function of the `tibble` package.

```{r}
land <- readxl::read_excel(here::here("data", "LND01.xls"))
land <- as_tibble(land)
```

We can take a look at the data using the base `head()` function which will show the first 3 rows.

```{r}
head(land, n=3)
```

AVOCADO: maybe a sentence on what is actually in the dataset? it looks like both state level (e.g. alabama, but also non-state level) info? 

Looks good!

```{r, echo = FALSE, eval = FALSE}
write.csv(land, file = here::here("data", "county_land_area.csv"))
save(land, file =  here::here("data", "county_land_area.rda"))
```


## Accessing data with APIs

The `httr` package creates what are called "GET requests" so that we can make these requests inside of R. This allows for the data to be retrieved from the API within R.

The `jsonlite` package allows you to convert the data from `JSON` (often used by APIs) to a different format that is easier to work with.

APIs typically require a password or key to gain access or authenticate your data request. The `httr` package helps to authenticate your data request. Often these keys are something that you do not want to share, unless the API is public.

In our case the [API](https://arcos-api.ext.nile.works/__swagger__/) is indeed public, and currently "uO4EK6I" is publicly published as a key to use on the [github page](https://github.com/wpinvestigative/arcos) for the `arcos` package. We will use that here to access the API.


## Population Data

We are interested in the county level data - first let's get the population data. We can access it by:

1. Pressing the `GET` button on the API.

```{r, echo=FALSE}
knitr::include_graphics(here::here("img", "get.png"))

```

2. Pressing the "Try it out" button.

```{r, echo=FALSE}
knitr::include_graphics(here::here("img", "tryitout.png"))
```

3. Entering the key (which we got from [here]([github page](https://github.com/wpinvestigative/arcos))).

```{r, echo=FALSE}
knitr::include_graphics(here::here("img", "key.png"))
```

4. Clicking the "Execute" button.

```{r, echo=FALSE}
knitr::include_graphics(here::here("img", "execute.png"))
```

This gives us the following output:

`curl -X GET "https://arcos-api.ext.nile.works/v1/county_population?key=uO4EK6I" -H  "accept: application/json"`

We can copy the URL section `"https://arcos-api.ext.nile.works/v1/county_population?key=uO4EK6I"` and use it in the `GET()` function of the `httr` package :

```{r}
count_url <- "https://arcos-api.ext.nile.works/v1/county_population?key=uO4EK6I"

county_pop_json <- 
  httr::GET(url = count_url)
```

If we needed to specify a username and password, we would do so using the `authenticate()` function of the `httr` package within the `GET` function. The `authenticate()` function takes `user`, `password` and `type` arguments.

Here is an example:

```{r, eval = FALSE}
GET(url = "https://exampleURL", 
    authenticate(user = "username", 
                 password = "password", 
                 type = "basic"))
```

The default authentication request type is `"basic"` and typically what is needed.

Now that we have used the `GET` function, let's see what the data are. 

```{r}
county_pop_json
```

Here we can see that the object called `countyjson` is a `json` object. 
A [JavaScript Object Notation (JSON)](https://fileinfo.com/extension/json#:~:text=A%20JSON%20file%20is%20a,web%20application%20and%20a%20server.) object (or file format) is [lightweight](https://en.wikipedia.org/wiki/Lightweight_programming_language) meaning that it does not take up much memory and they are human readable files to make transmitting data from websites easier.

You will also see that the `Status` is `200`, which means that we were successful in retrieving the data from the API.

Now we can use the `content()` function of the `httr` package to extract the text from the file:

```{r}
county_pop_text <- 
  httr::content(county_pop_json, "text")
```

This will be a very large string at this point, we can take a look at part of it by using the `str_sub()` function of the `stringr` package. In this case we will only look at the first 400 characters.

What is a string or a character?

***
<details> <summary> Click here for an explanation about character strings if you are not yet familiar </summary>

There are several classes of data in R programming. 
Character is one of these classes. 
A character string is an individual data value made up of characters. 
This can be a paragraph, like the legend for the table, or it can be a single letter or number like the letter `"a"` or the number `"3"`. 

If data are of class character, than the numeric values will not be processed like a numeric value in a mathematical sense. 

</details>
***

```{r}
stringr::str_sub(county_pop_text, start = 1, end = 400)
```


To get the data into a more readable format, we can use the `fromJSON()` function of the `jsonlite` package and again create a tibble of the data using `as_tibble()` 

```{r}
county_pop <- 
  jsonlite::fromJSON(county_pop_text, 
                     flatten = TRUE)

county_pop <- as_tibble(county_pop)
```

We can use the `glimpse()` function and the `distinct()` function of the `dplyr` package to get a better sense of the data. The `distinct()` function allows us to take a look at the unique values of the `year` variable.

```{r}
dplyr::glimpse(county_pop)
dplyr::distinct(county_pop, year)
```

It looks like we have the full data from 2006-2014.


```{r, eval = FALSE, echo = FALSE}
write.csv(county_pop, file = here::here("data", "county_pop_arcos.csv"))
save(county_pop, file =  here::here("data", "county_pop_arcos.rda"))
```


## Annual Shipment Data

We are also interested in opioid pill shipment data at the county level. 

Here is the result of the same steps using the API for the `combined_county_annual` data:

```
curl -X GET "https://arcos-api.ext.nile.works/v1/combined_county_annual?key=uO4EK6I" 
     -H  "accept: application/json"
```     

or in R, the URL is 

```{r}
annual_url <- 
  "https://arcos-api.ext.nile.works/v1/combined_county_annual?key=uO4EK6I"
```


#### {.recall_code_question_block}
<b><u> Question Opportunity </u></b>

See if you can modify the import code without looking at the code for the population data.

####

***

<details> <summary> Click here to reveal the code. </summary>

```{r, eval = FALSE}
county_annual_json <- 
  httr::GET(url =  annual_url)

county_annual_json_text <- 
  httr::content(county_annual_json, "text")

county_annual <- 
  jsonlite::fromJSON(county_annual_json_text, flatten = TRUE)

annualDosage <- 
  tibble::as_tibble(county_annual)
```

</details>

***

```{r, echo = FALSE, eval = FALSE}
write.csv(annualDosage, file = here::here("data", "county_annual.csv"))
save(annualDosage, file =  here::here("data", "county_annual.rda"))
```

```{r, echo = FALSE}
load(here::here("data", "county_annual.rda"))
```

Now let's take a look at the data:
```{r}
glimpse(annualDosage)
distinct(annualDosage, year)
```

Looks like we have the same years of data.

```{r, eval = FALSE, echo = FALSE}
# We also retrieved the monthly data for instructors who wish to use this data
countyjson <- 
  httr::GET(url = "https://arcos-api.ext.nile.works/v1/combined_county_monthly?key=uO4EK6I")

countyjson_text <- 
  httr::content(countyjson, "text")

county <- 
  jsonlite::fromJSON(countyjson_text, flatten = TRUE)

monthlyDosage <- 
  tibble::as_tibble(county)

write.csv(monthlyDosage, 
          file = here::here("data", "county_monthly.csv"))

save(monthlyDosage, 
     file =  here::here("data", "county_monthly.rda"))
```


# **Data Exploration**
***

```{r, echo = FALSE}
# In case an instructor wants to start here we will load the data

load(here::here("data", "county_land_area.rda"))
load(here::here("data", "county_pop_arcos.rda"))
load(here::here("data", "county_annual.rda"))
```


Now let's take a deeper look at the data to see if we have any missing data using the `naniar` package.

We can use the `vis_miss()` function to create a plot of missing data.

Let's start with the land area data.
```{r}
naniar:: vis_miss(land)
```
Looks like there is no missing data.

How about the population data:

```{r}
vis_miss(county_pop)
```

Although it is very hard to see in the figure, we appear to be missing some values for the `NAME` and `variable` data, but we don`t intend to use these, so this should be OK. It is however a good idea to check these rows to see if anything strange is happening.

Let's use the `filter()` function of the `dplyr` package and the `is.na()` base function to see more about the data that does not have `NAME` values.

We will also start using the `%>%` pipe of the `magrittr` package for our assignments.

***
<details> <summary> Click here if you are unfamiliar with piping in R, which uses this `%>%` operator</summary>  


By [piping](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html){target="_blank"} we mean using the `%>%` pipe operator which is accessible after loading the `tidyverse` or several of the packages within the tidyverse like `dplyr` because they load the [`magrittr` package](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html){target="_blank"}. 
This allows us to perform multiple sequential steps on one data input. 

</details> 
***

```{r}
county_pop %>% filter(is.na(NAME))
```

This looks OK. So let's now move on to the DEA data.
```{r}
vis_miss(annualDosage)
```

Interesting, we appear to be missing `countyfips` codes for a small percentage of our annual data.


Let's take a look at this data:

```{r}
annualDosage %>% filter(is.na(countyfips))
```


We can see that the first rows with missing data for `countyfips` is data for Puerto Rico - it might make sense for Puerto Rico to be missing these codes.

Let's see if there is any data other than data for Puerto Rico that is also missing `countyfips` values. We can use the `!=` operator which indicates not equal to.


```{r, eval = FALSE}
annualDosage %>% filter(is.na(countyfips)) %>%
 filter(BUYER_STATE != "PR")
```

#### {.scrollable }
```{r, echo = FALSE}
annualDosage %>% filter(is.na(countyfips)) %>%
 filter(BUYER_STATE != "PR") %>%
  # this allows us to show the full output in the rendered rmarkdown
 print(n = 1e4)
```
####

It looks like there is also data for other territories in the dataset, as well as some counties with no name.

For some reason the rows for the Montgomery county of Arkansas are also missing a `countyfips` value.

```{r}
annualDosage %>% filter(is.na(countyfips)) %>%
 filter(BUYER_STATE == "AR")
```

According to this [website](https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697) the FIPS code is 05097.


We will update these values in the next section.


# **Data Wrangling**
***

```{r, echo = FALSE}
# In case an instructor wants to start here we will load the data

load(here::here("data", "county_land_area.rda"))
load(here::here("data", "county_pop_arcos.rda"))
load(here::here("data", "county_monthly.rda"))
```

## Cleaning land data

We want the `LND110210D` column which is the data from the year 2010.

LND = Land Area
110 = unit square miles (subgroup-code of the group) * avocado I found this somewhere else.. the census info was vague would like to confirm that that is indeed what the subgroup code shows us
2 = century
10 = 2010 (based on the century)
D = Data

Thus we can select just the county names, the county numeric codes, and the `LND110210D`column by using the `select()` function of the `dplyr` package.

```{r}
county_area <- land %>% select(Areaname, STCOU, LND110210D)
county_area
```

AVOCADO: again to the point above, there are more than just counties in this data set, i.e., it contains state data as well as the full US. Should we try to filter to this here, or will that happen automatically later?

We can see that in addition to county-level measurements, we also have state-level and full US measurements of land area. But we will filter this data out down below.


## Updating `countyfips`

We will use the `case_when()` function of the `dplyr` package recode the `NA` values for `countyfips` for the rows for the `MONGOMERY` county of `AR` to be `05097`. First we need to identify these particular rows. Because Montgomery may be a county name in other states, we need to evaluate when the `BUYER_STATE` is `AR` and when the `BUYER_COUNTY` is `MONTGOMERY`. We will use the `&` operator to indicate that both conditions must be true. We will then recode the `coutryfips` values for these rows to be `"05097"` using the `~` symbol. All other values need to stay the same. Thus we need to use `TRUE ~` to recode all the other `countyfips` values to what they currently are. Otherwise these would automatically be set to `NA`.

We are also going to use a special pipe operator from the [`magrittr` package](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html) called the *compound assignment pipe operator* or sometimes the *double pipe operator*. 

This allows us to use the `annualDosage` as our input and, in the same step, reassign it to take the updated value of the tibble, after all the subsequent steps have been performed. In this case, it is only one step, but this saves us typing and avoids creating an intermediate version of the tibble.

```{r}
annualDosage %<>% 
  mutate(countyfips = case_when(BUYER_STATE == "AR" & 
                               BUYER_COUNTY == "MONTGOMERY" ~ as.character("05097"),
                               TRUE ~ countyfips))
```

Now we can check that we indeed fixed our data.

```{r}
annualDosage %>% 
  filter(is.na(countyfips)) %>%
  filter(BUYER_STATE == "AR")

annualDosage %>% 
  filter(BUYER_COUNTY == "MONTGOMERY") %>%
  filter(BUYER_STATE == "AR")
```

Great! We fixed it.


OK, we also had some rows that didn't have county names because they were just missing or the data was for US territories. We will remove the values that don't have county names.

First let's take a look at them again.

```{r}
annualDosage %>% filter(is.na(BUYER_COUNTY))
```

We can remove these rows from our data set using the `filter` command along with the `!` (exclamation poing) before the `is.na()` function.

```{r}

annualDosage %<>%  filter(!is.na(BUYER_COUNTY))
```

And now let's check that these `NA` values are gone:

```{r}
annualDosage %>% filter(is.na(BUYER_COUNTY))

```

As we  have seen, our `annualDosage` object has data for US territories. So let's check to see if our land area data also has information for US territories, which would allow us to investiage them as well, a potentially interesting addition to our analysis. If not, we will remove the data for the territories in our `annualDosage` data. As a first case, we can use the `str_detect()` function of the `stringr` package, which contains lots of functions for looking for patterns in character strings, to look for data from Puerto Rico. 

The `str_detect()` function allows us to look for a particular pattern. It does not have to be the full value, i.e., a partial match is allowed. Thus we can look to see if there are any `PR` strings within the values of the the `Areaname` variable. 

```{r}
county_area %>% filter(str_detect(string = Areaname, pattern = "PR"))
```

It does not look as if there are any entries for Puerto Rico, but let's check our code. You can see using a different abbreviation, that this code works as intended:

```{r}
county_area %>% filter(str_detect(string = Areaname, pattern = "AR"))
```

OK, so it does mot look like there is any territory land area data in this dataset. Thus we will also remove these from the `annualDosage` and `monthlyDosage` tibbles.

#### {.recall_code_question_block}
<b><u> Question Opportunity </u></b>

Do you recall how to do this?


####


<details> <summary> Click here to reveal the code. </summary>
```{r}
annualDosage %<>% filter(!is.na(countyfips))
```
</details> 

AVOCADO: What about the `monthlyDosage` object?

```{r}
naniar:: vis_miss(annualDosage)
```

Great! Now there is no missing data in our annual data.


## Rural and Urban Counties

Defining if a region is rural or urban is actually quite complicated as overall population, the structure of our towns and cities, and the access between different locations all change over time. Please see this [report](https://www2.census.gov/geo/pdfs/reference/ua/Defining_Rural.pdf) form the US Census Bureau about the history of this definition. 

According to several [definitions](https://www.hrsa.gov/rural-health/about-us/definition/index.html) - urban **areas** are often defined as those with greater than 50,000 people. However, there are also
[definitions](https://www.ers.usda.gov/topics/rural-economy-population/rural-classifications/what-is-rural/) of rural areas being based on  "population densities of less than 500 people per square mile and places with fewer than 2,500 people". Typically counties are made up of multiple areas, making it complicated to assign a single "rural" or "urban" label at the county level. 

The census redefines rural and urban areas around the US relatively often. However, census collections about these measurements do not occur every year.

Thus we will define a county as rural or urban based on the population density using the USDA [definition]((https://www.ers.usda.gov/topics/rural-economy-population/rural-classifications/what-is-rural/)) that we described above:

1) **rural**  = population densities of less than 500 people per square mile, as well as places with fewer than 2,500 people     
2) **urban** = population densities of greater than 500 people per square mile  

Ideally we would want land area from each year, as these do fluctuate a bit, however, the 2010 data should be a decent approximation as 2010 is in the middle of our time span.

We will therefore calculate the density as the number of people per square mile by dividing the population values by the land area values. To do so we first need to combine our `county_area` and our `county_pop` data together. First we want to make sure that we have one column, in our case the column that contains the numeric code for the counties, in the same format and with the same name in both the tibbles that we wish to combine. 

We can use the `rename()` function of the `dplyr` package to rename the `STCOU` column from the `county_area` data set to be `countyfips`. The new name is always listed first before the old name with this function like so: `rename(new_name = old_name)`.

```{r}
county_area %<>%
  rename(countyfips = STCOU)
```


We can use the `mutate()` function of the `dplyr` package to make the `countyfips` variable a factor in both tibbles. 

AVOCADO: Why do these variables need to be factors for us to combine them? I believe you can join based on a character valued variable? 

What exactly is a factor?

***
<details> <summary> Click here for an explanation of data classes in R </summary>

There are several classes of data in R programming. 
Character is one of these classes. 
A character string is an individual data value made up of characters. 
This can be a paragraph, like the legend for the table, or it can be a single letter or number like the letter `"a"` or the number `"3"`. 

If data are of class character, than the numeric values will not be processed like a numeric value in a mathematical sense. 

If you want your numeric values to be interpreted that way, they need to be converted to a numeric class. 
The options typically used are integer (which has no decimal place) and double precision (which has a decimal place). 

A variable that is a [factor](https://www.stat.berkeley.edu/~s133/factors.html#:~:text=Conceptually%2C%20factors%20are%20variables%20in,refered%20to%20as%20categorical%20variables.&text=Factors%20in%20R%20are%20stored,when%20the%20factor%20is%20displayed.) has a set of particular values called levels. Even if these are numeric, they will be interpreted as levels (i.e., as if they were characters) not as mathematical numbers. The values of a factor are assumed to have a particular ordering; by default the order is alphabetical, but this is not always the correct/intuitive ordering. You can modify the order of these levels with the `forcats` package.

</details>


***


```{r}
county_pop %<>%
  mutate(countyfips = as.factor(countyfips))

county_area %<>%
  mutate(countyfips = as.factor(countyfips))
```

Great! Now we are ready to combine our data together.

We can do so using one of the  `*_join()`functions of the `dplyr` package.

There are several ways to join data using the `dplyr` package.


```{r, echo = FALSE, outwidth = "50%"}
knitr::include_graphics(here::here("img", "join.png"))
```

##### [[source]](https://dplyr.tidyverse.org/reference/join.html)

Here is a visualization of these options:

```{r, echo = FALSE, outwidth = "40%"}
knitr::include_graphics(here::here("img", "join_image.png"))
```

##### [[source]](https://rstudio.com/resources/cheatsheets/)

See [here](https://dplyr.tidyverse.org/reference/join.html) for more details about joining data.

Since the population data came from the API, we probably have information about opioid pill shipments for each of the included counties. Since the land area data came from a different source, it may contain additional counties that are not in our population or drug shipment data.  Thus we will use the `left_join()` function where x in this case will be the `county_pop`  and y will be the `country_area`. Thus we will add the `LND110210D` (land area) values for all counties that match `county_pop` based on the `countyfips` column that they have in common. 


```{r}
county_info <-left_join(county_pop, county_area)
```

You can check to see that the `county_info` tibble has the same number of rows as the `county_pop` tibble, i.e., our join preserved the data points in the `county_pop` tibble and added values from the `county_area` tibble where there was a match on the `countyfips` variable.

We are now ready to calculate the population density per square mile. We can create a new column with this data using the `mutate()` function and the `/` to divide the `population` value by the land area value (in square miles) for each county. Let's also make the `year` variable a factor.

```{r}
county_info %<>%
  mutate(density = population/LND110210D,
         year = as.factor(year))

glimpse(county_info)
```


Great, now we are ready to create a variable that classifies if a county was rural or urban based on our definition of rural counties being those with less than 500 people per square mile as well as those with less than 2,500 people. We will use the `case_when()` function of the `dplyr` package to classify the new `rural_urban` variable as either `"Urban"` or `"Rural"` based on the evaluations of the `density` and the `population` variables. If the density is greater than or equal to 500 people per square mile, then the county will be coded as `"Urban"`, alternatively if the density is less than 500 people per square mile or the population is less than 2500, than the county will be coded as `"Rural"`. The `|` operator is used to indicate that either expression should result in coding the county as `"Rural"`

```{r}

county_info %<>%
  mutate(rural_urban = case_when(density  >= 500 ~ "Urban",
                                 density  < 500 | population < 2500 ~ "Rural"))
```

We can use the `count()` function of the `dplyr` package to see how many of each this resulted in:

```{r}
count(county_info, rural_urban)
```

We will now combine the `annualDosage` data with the `count_info` tibble.

#### {.think_question_block}
<b><u> Question Opportunity </u></b>

How might we do this?

####


<details> <summary> Click here to reveal the code. </summary>

```{r}
annualDosage %<>%
  mutate(countyfips = as.factor(countyfips),
                  year = as.factor(year))
  
Annual <-left_join(annualDosage, county_info)

```
</details>

```{r}
glimpse(Annual)
```

Great, now we should have the data that we need for the case study. But we will want to do some additional checks before we proceedd with our analysis.


Notice how there is a variable called `DOSAGE_UNIT`. This variable gives the number of pills shipped to a pharmacy in this county that were either [oxycodone](https://www.dea.gov/sites/default/files/2020-06/Oxycodone-2020_0.pdf) or [hydrocodone](https://www.deadiversion.usdoj.gov/drug_chem_info/hydrocodone.pdf).

Let's do a check to see how complete our data is now that we have combined our `country_info` data with the `monthlyDosage` and `annualDosage` data. We will have NA values for any counties present in the DAE data but not in our land area data. We can use the `vis_miss()` function from thee `naniar` package` to create a plot that shows if we have any missing data.

```{r}
naniar:: gg_miss_var(Annual)
```

We can see that several variables are missing for some of our counties.

AVOCADO: I'm not following why you are filtering on NA for STATE. I would think it would make more  intuitive sense to filter on NA for density for example, since that is relevant to our analysis.

#### {.scrollable }

```{r}
Annual %>%
  filter(is.na(STATE))
```
####

There does not appear to be land area and/or population data for these counties.

AVOCADO: I'm not really sure what the goal of this code chunk is. Most of these subgroups do not have any data at all in the table as far as I can see. Are we checking back in the original data tibbles (prior to merging) to see where the data first went missing? 
```{r}
county_info %>% filter(countyfips == "01001") # example of other data that does have values
county_info %>% filter(countyfips == "05097")
county_info %>% filter(countyfips == "02201")
county_info %>% filter(countyfips == "02280")

# there is land data for this county but thats all
land %>% filter(STCOU == "05097")
land %>% filter(STCOU == "02201")
land %>% filter(STCOU == "02280")

county_pop %>% filter(countyfips == "05097")

county_pop %>% filter(countyfips == "05097")
county_pop %>% filter(BUYER_COUNTY == "MONTGOMERY"& BUYER_STATE =="AK")
```

We will now remove these rows before further analysis:

#### {.recall_code_question_block}
<b><u> Question Opportunity </u></b>

Do you recall how you would do this?

####


<details> <summary> Click here to reveal the code. </summary>

```{r}

Annual %<>% filter(!is.na(STATE))

```

</details>

```{r}
naniar:: gg_miss_var(Annual)
```

Nice! Now we have no missing data.

Let's also check if there were any counties in `county_info` that were not in the DEA `annualDosage` data.

AVOCADO: I think this code chunk could also use some more explanation.


```{r}
checking <-left_join(county_info, annualDosage)
gg_miss_var(checking)
checking %>%
  filter(is.na(DOSAGE_UNIT)) 

checking %>%
  filter(is.na(DOSAGE_UNIT)) %>% 
  distinct(countyfips) 

annualDosage %>% filter(BUYER_COUNTY == "BORDEN")
annualDosage %>% filter(BUYER_COUNTY == "COKE")
```

There are 174 counties that don't have any data in the DEA data. It is unclear why these counties are not included case.  A Google search of the [Borden](https://en.wikipedia.org/wiki/Borden_County,_Texas) and [Coke](https://en.wikipedia.org/wiki/Coke_County,_Texas) counties in Texas does not indicate anything usual about the counties in terms of when it was established or if it became part of another county later in time. It is important to keep in mind as we continue to analyze this data, that the ARCOS data from the DEA released by the Washington Post does not include pill shipment information for all US counties. 

```{r,echo= FALSE, eval = TRUE}
write.csv(Annual, file = here::here("data","Wrangled", "Annual_opioid_data.csv"))
save(Annual, file =  here::here("data","Wrangled", "Annual_opioid_data.rda"))
write.csv(county_info, file = here::here("data", "Wrangled", "county_info.csv"))
save(county_info, file = here::here("data", "Wrangled", "county_info.rda"))
```

# **Data Visualization**
***

```{r,echo= FALSE, eval = TRUE}
# In case instructors wish to start here, we will load the necessary data

load(file =  here::here("data","Wrangled", "Annual_opioid_data.rda"))
load( file = here::here("data", "Wrangled", "county_info.rda"))
```

We will begin by taking a deeper look at our data with some visualizations. We will use the `ggplot2` package to create these visualizations.

***
<details><summary> Click here for an introduction about this package if you are  new to using `ggplot2` </summary>

The [ggplot2 package](http://ggplot2.tidyverse.org) is generally intuitive for beginners because it is based on a  [grammar of graphics](http://vita.had.co.nz/papers/layered-grammar.html) or the `gg` in `ggplot2`. 
The idea is that you can construct many sentences by learning just a few nouns, adjectives, and verbs. There are specific “words” that we will need to learn and once we do, you will be able to create (or “write”) hundreds of different plots.

The critical part to making graphics using `ggplot2` is the data needs to be in a _tidy_ format. 
Given that we have just spent time putting our data in _tidy_ format, we are primed to take advantage of all that `ggplot2` has to offer! 

We will show how it is easy to pipe _tidy_ data (output) as input to other functions that create plots. 
This all works because we are working 
within the _tidyverse_. 

**What is the `ggplot()` function?** 
As explained by Hadley Wickham:

> The grammar tells us that a statistical graphic is a mapping from data to aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars). The plot may also contain statistical transformations of the data and is drawn on a specific coordinates system.

`ggplot2` Terminology: 

- **ggplot** - the main function where you specify the dataset and variables to plot (this is where we define the `x` and
`y` variable names)
- **geoms** - geometric objects
    - e.g. `geom_point()`, `geom_bar()`, `geom_line()`, `geom_histogram()`
- **aes** - aesthetics
    - shape, transparency, color, fill, line types
- **scales** - define how your data will be plotted
    - continuous, discrete, log, etc

The function `aes()` is an aesthetic mapping function inside the `ggplot()` object. 
We use this function to specify plot attributes (e.g. `x` and `y` variable names) that will not change as we add more layers.  

Anything that goes in the `ggplot()` object becomes a global setting. 
From there, we use the `geom` objects to add more layers to the base `ggplot()` object. 
These will define what we are interested in illustrating using the data.

</details>

***

## Population density

Let's make a plot to see how population density has changed over time in each state.

To do so we want to calculate a mean population density (across all the counties) for each state for each year. 

We can do this using the `group_by()`  and `summarize()` functions of the `dplyr` package. The `group_by` functions allows for the data to be arranged into groups for subsequent functions. 

Thus, if we group only by state using the following code, you will see that this results in 51 groups (one for each state including Washington DC). This doesn't change anything about the data itself (or even how it is printed aside from the groups written above the table), just how it will be handled in subsequent steps.

```{r}
Annual  %>% group_by(BUYER_STATE)
```

Alternatively, if we group by year this results in 9 groups of data, one for each year.

```{r}
Annual  %>% group_by(year)
```

We want to group by both `BUYER_STATE` and `year`, so that we get the mean of all the counties for each state for each year. If we only did by state, we would only get 51 summarized results, one for each state representing a mean across the years. 

```{r}
Annual  %>% group_by(BUYER_STATE, year)
```
We can see that this results in 459 groups. This makes sense because 51 groups over 9 years is 51 multiplied by 9, which equals 459. 

We can then use the `summarize` function to create a new variable called `sum_DENS` which will be equal to the mean of the `density` variable for all the counties within each of the 449 groups. If we had missing values we would need to use the `na.rm = TRUE` argument to remove any missing values in our calculation.

```{r}
Annual %>% group_by(BUYER_STATE, year) %>%
     summarize(mean_DENS = mean(density, na.rm = TRUE))
```

OK! Now we are ready to make our first plot. 

We will start with the `ggplot()`function to specify what variables will be used for the x-axis and y-axis, as well as if any variable should be used to specify different colors on the plot. This will result in a blank plot.  Then we need to use a `geom_*` function to specify what type of plot we would like to make. 

If you type `geom_` into the console of RStudio, you will see a list of options. 
```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics(here::here("img", "geom.png"))
```

We will create a scatter plot using the `geom_point()` function. We will also use the `theme_minimal()` function to change the overall aesthetics of the plot. See [here](https://ggplot2.tidyverse.org/reference/ggtheme.html) for a list of options.

We will also use the `theme()` function to further specify how we want the plot to be displayed. We would like the x axis text to be angled by 90 degrees. We can use the `element_text()` function to change aspects about the text. and we can use the `axis.text.x` argument to specify that we want to specifically change the text of the x axis. You can type `theme(` in the RStudio console and press tab to see a list of argument options for things that you can change in your plot. 

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics(here::here("img", "theme.png"))
```
Finally, we can use the `labs()` function of the `ggplot2` package to specify the labels of the plot. 

```{r}
Annual %>% group_by(BUYER_STATE, year) %>%
  summarize(mean_DENS = mean(density, na.rm = TRUE)) %>%
  ggplot(aes(x = BUYER_STATE, y = mean_DENS, col = year)) + 
    geom_point() + 
    theme_minimal()+
    theme(axis.title.x = element_blank(),
           axis.text.x = element_text(angle = 90)) +
    labs(x = "State",
     title =  "Mean County Population Density of each State",
         y = "Mean Population Density (People per square mile)")
```

We can see that the average state population density is fairly similar for most states. However DC, MA, NJ, NY, RI, and VA have much higher average county densities. We also see that DC shows the largest change over time, as we can see the other individual points for each year. For other states the change was so small that they are overlapping.

What about overall population density, how did the national average of all US counties change?

We will ignore the different states in this case and we will calculate the mean of all US counties for each year.


#### {.think_question_block}
<b><u> Question Opportunity </u></b>

How might you create this plot?

####

***

<details> <summary> Click here to reveal the code. </summary>

```{r}
USavg_dens <-Annual %>% group_by( year) %>%
     summarize(mean_DENS = mean(density)) %>%
     ggplot(aes(x = year, y = mean_DENS)) + 
        geom_point() +
        theme_minimal() +
        theme(axis.title.x=element_blank()) +
        labs(title = "US mean population density",
                 x = "year",
                 y = "population density (people per square mile)")
```

In this case we saved the plot to an object called `USavg_dens`.

</details>
***


```{r}
USavg_dens
```


Overall the density has increased, if you take a look at the y-axis you can see that the density has changed by about 13 people per square mile from 2006 to 2014. 

How does this compare with raw population values?

To make this plot, we will take the sum of the population values for each county rather than the mean. 


#### {.think_question_block}
<b><u> Question Opportunity </u></b>

How might you create this plot?

####

***

<details> <summary> Click here to reveal the code. </summary>


```{r}

overall_density <-Annual %>% 
  group_by(year) %>%
  summarise(total_population = sum(population)) %>%
     ggplot(aes(x =year, y = total_population)) + 
        geom_point() +
        geom_smooth() +
        theme_minimal() +
        theme(axis.title.x = element_blank()) +
        labs(title = "US Population from 2006-2014",
                 x = "year",
                 y = "US total population")
 
```

</details>

```{r}
overall_density
```

## Rural and Urban areas

How have the number of rural and urban areas changed over years?

To determine how the number of each type of county has changed over time, we will use the `count()` function of the `dlyr` package after grouping by the `year` variable to count the number of occurrences of the unique values (which are `Rural` and `Urban`) in the `rural_ubran` variable.


```{r}
Annual %>% 
  group_by(year) %>%
  count(rural_urban)
```

In this case we can make a plot using two different `geom_*` layers together. Whatever `geom_*` layer is added last will be displayed on top. In this case we will use `geom_point()` and `geom_smooth()` to add a line connecting the points of the scatter plot of the `geom_point()` function. We can also use the `facet_wrap()` function of the `ggplot2` package to create subplots based on the `rural_urban` variable. Thus we will create a subplot for the `Rural` values and another for the `Urban` values.

The `scales = "free"` argument allows for each to have a different scale for the y-axis.


```{r}
 Annual  %>% group_by( year) %>%
      count(rural_urban) %>%
 ggplot(aes(x = year, y = n, col = rural_urban, group = rural_urban)) + 
  geom_point() + 
  geom_smooth() +
  facet_wrap(~ rural_urban, scales = "free") +
  theme_minimal() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 90),
        legend.title = element_blank()) +
  labs(y = "Number of Counties", 
       x = "Year",
   title = "Change in the number of the type of county in the US over time")
```

As one might expect, it looks like the number of urban areas has increased, while the number of rural areas has decreased over time.


Let's also create a table to look at the number of rural and urban counties over time. To do this we can use the package `formattable`. First we need to get the data into the format that we would like. We previously counted the number of `Rural` and `Urban` counties for each year. However, the data was presented in a format that is called [long format](https://en.wikipedia.org/wiki/Wide_and_narrow_data). In this format, variables that could possibly be presented as separate columns are condensed into fewer columns, while still maintaining only a single value per cell. The opposite of this format is called [wide format](https://en.wikipedia.org/wiki/Wide_and_narrow_data) data, which therefore has more columns and fewer rows. This is best illustrated with an example.

Here you can see wide data on the left in the following image with more columns and fewer rows and long data on the right where the month columns have been collapsed into two longer columns (one with the name of the month and one with the numeric value) resulting in fewer columns and more rows. While long format is very useful for creating plots with `ggplot2` it is helpful to have the data in wide format for tables that someone would quickly read, which is our current goal.


```{r, echo = FALSE}
knitr::include_graphics("https://flourish.studio/images/blog/wide-to-long.png")
```

#### [[source]](https://flourish.studio/images/blog/wide-to-long.png)
***
<details> <summary> Click here to see another example. </summary>

Here is an example of wide data about different measurements of a variety species of Iris flowers. 
```{r, echo = FALSE}
set.seed(123)
wide1 <-slice_sample(iris, n = 10)
wide1
```

OK, so currently we have 4 different columns about measurements of different flowers. Since all of these measurements are similar, one might produce a new variable that is made up of the names of the first four variables and another that is the numeric value like so:

```{r, echo = FALSE}
long <-pivot_longer(wide1, cols = -Species, names_to = "Measurement", values_to = "Value")
long
```

</details>
***

Now we will demonstrate how to make the counts of `Rural` and `Urban` data into wide format from long format. Here is our original data:

```{r}
Annual  %>% 
  group_by(year) %>%
  count(rural_urban)
```

We would like the `rural_urban` data to be shown in two different columns; one that shows `Rural` counts and one that shows `Urban` counts, as this would be easier for people to read. We can use the `pivot_wider()` function of the `tidyr` package to do this. This takes two important arguments:

1) `names_from`  - this argument indicates what variable to use to create the names of the new variables
2) `values_from` - this argument indicates what variable to use to fill in the values of the new variables

In our case we will obtain the names from the `rural_urban` variable and the values from the `n` variable.

```{r}
Annual  %>% 
  group_by( year) %>%
  count(rural_urban) %>%
  tidyr::pivot_wider(names_from = rural_urban, 
                    values_from = n)
```

Nice!

Now, let's also create two new variables that show the change in count of rural and urban counties from one year to the next. We can do so using the `lag()` function of the `dplyr` package. This function will find the previous value thus `Rural- lag(Rural)` will take the current row and subtract the previous row's value. Note that is necessary to include the `ungroup()` function to stop grouping by year.

```{r}

Annual%>% 
  group_by(year) %>%
  count(rural_urban) %>%
  tidyr::pivot_wider(names_from = rural_urban,
                     values_from = n) %>%
  ungroup() %>% 
  mutate("Rural Change" = Rural - lag(Rural), 
         "Urban Change" = Urban - lag(Urban))
```

Let's also add a column about the percent urban for each year. We will use the base `round()` function to round the percentages to 2 digits after the decimal using the `digits = 2` argument. Finally, we also rename the `year` variable to be `Year` using the `rename()` function of the `dplyr` package, which requires that the new name be listed before the `=` sign followed by the old name. 

```{r}

R_U <-Annual%>% 
  group_by(year) %>%
  count(rural_urban) %>%
  tidyr::pivot_wider(names_from = rural_urban,
                     values_from = n) %>%
  ungroup() %>% 
  mutate("Rural Change" = Rural - lag(Rural), 
         "Urban Change" = Urban - lag(Urban),
        "Percent Urban" = round((Urban/(Urban + Rural))*100, digits = 2)) %>%
  rename("Year" = "year")

R_U
```
Nice, now we have a pretty easy to interpret table, but we can make it even easier to quickly assess trends in the data using the `formattable` package. The `formmattable()` function creates a formatted table, and takes a list of variables and a stylized version of each variable in which to add special formatting.  As a simple example, we will use the `color_bar()` function of this package to add color bars to the `percent_urban` column which shows changes in values by the width of a color bar.

```{r}

formattable::formattable(R_U, list(`Percent Urban` = formattable::color_bar("#FA614B")))
```

Nice, now we can see how much the percentage has changed over time. 

We also use the `formatter()` function to change the color of the `Rural Change` and `Urban Change` variables so that if the value is negative it will be red and if it is positive it will be green. 

The `formatter()` function takes an HTML style [tag](https://developer.mozilla.org/en-US/docs/Glossary/Tag) name which can be any character string (although generally one would use ["span"](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/span#:~:text=The%20HTML%20element%20is,attribute%20values%2C%20such%20as%20lang%20.) using the `.tag` argument  and a `style` argument where style aspects such as color can be specified using a `color` argument.

We will create a function called `redgreen` that will specify that if a value is less than zero it should be red and if it is greater than zero it should be green. To do this we will use the `case_when()` function like we did previously when creating our `rural_urban` variable of the `county_info` object in the [**Data Wrangling**] section.

To create a function we will use the base `function()` function. The inputs of the function are contained within the parentheses `()`, while the steps that should be performed on the input are contained within the curly brackets `{}`. In this case, our input will be called `number`.  Now when `redgreen` is used this will perform the `case_when` evaluation on the input provided.


```{r}
redgreen <- function(number){case_when(number < 0 ~"red", number > 0 ~ "green")}
```

We can see that this function does indeed change numeric values to be the color names `red` and `green`.
```{r}
redgreen(number = c(1,0,-1))
```

Now this function can be used to replace the `color` values for the `Rural Change` and `Urban Change` variables. 

```{r}
formattable(R_U, list(`Percent Urban` = color_bar("#FA614B"),
                      `Rural Change` = formatter(.tag ="span", 
                                                style = ~style(
                                                color = redgreen(number = `Rural Change`))),
                      `Urban Change` = formatter(.tag = "span", 
                                                style = ~style(
                                                color = redgreen(number = `Urban Change`)))))

```

Nice, now this is really easy to read.


## US shipments over time

Now let's get a sense of how the shipments of [oxycodone](https://en.wikipedia.org/wiki/Opioid_epidemic_in_the_United_States) and [hydrocodone](https://en.wikipedia.org/wiki/Opioid_epidemic_in_the_United_States) changed over time in general across all counties in the US.

First we are going to create a new variable in the `Annual` data that is the number of pills in millions, as this is easier for us to interpret.

```{r}
Annual %<>% 
  mutate(Pills_in_millions =  DOSAGE_UNIT/1000000)
```

Now, let's make a plot using the `stat_summary()` function rather than a `geom_*` function.

This allows us to calculate different features about the data to plot. Custom functions can be used or functions that are wrappers for the `smean.*` functions the `Hmisc` package. 

See the [documination](https://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf) for the `Hmisc` package to learn more about these functions.

ggplot2 wrapper   | Hmisc function | Details                                                                      
---------- |------------- | -------------------------------------
mean_cl_normal | smean.cl.normal | computes 3 summary variables: the
sample mean and lower and upper Gaussian confidence limits based on the t-distribution  
mean_sd | smean.sd | computes the mean plus or minus the standard deviation   
mean_sdl  | smean.sdl | computes the mean plus or minus a constant
times the standard deviation  
mean_cl_boot | smean.cl.boot | fast implementation of the basic nonparametric bootstrap for obtaining confidence limits for the population mean without assuming normality (default is 1000 bootstrap samples)
meadian_hilow | smedian.hilow | computes the sample median and a selected pair of outer quantiles having equal tail areas  


We will use the `mean_cl_boot()` function to create our plot. This will create a point at the mean and then lines around the point based on "confidence limits". In this case, the package uses a nonparametric bootstrap to compute a 95 % [confidence interval](https://en.wikipedia.org/wiki/Confidence_interval) for the mean. 

What does this mean?

According to [Wikipedia]((https://en.wikipedia.org/wiki/Confidence_interval), a confidence interval is:

> A range of plausible values for an unknown parameter (for example, the mean)

We don't know the true mean for the number of pills shipped to a US county, our data represents an estimate as we saw that not every county is represented in the data and it is highly possible that not all pill shipments are accounted for. 

Thus confidence intervals show the reliability of the mean estimate. If the interval is very large, than the true mean could possibly be many other values. 

Again according to [Wikipedia]((https://en.wikipedia.org/wiki/Confidence_interval):

> The interval has an associated confidence level that the true parameter is in the proposed range. 

> There is a 95% probability that the calculated confidence interval from some future experiment encompasses the true value of the population parameter.

It does not mean:

> A 95% probability that the interval covers the population parameter


Rather it is that we are 95% percent **confident** that the interval covers the population mean.

It is typically calculated as follows:

$\bar{x} +- Z \frac{s}{\sqrt{n}}$

Where \bar{x} is the observed mean, $s$ is the standard deviation, $n$ is the number of observations, and $Z$ is the Z score from the normal distribution for the particular confidence level chosen which is typically 95%. For example with a 95% confidence level the Z is 1.96 indicating that the area around the mean on the [sampling distribution](https://en.wikipedia.org/wiki/Sampling_distribution) is within plus or minus 1.96 standard deviations.

This is best demonstrated with an image of the [sampling distribution](https://en.Wikipedia.org/wiki/Sampling_distribution:

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "dist.png"))
```

##### [[source]]((http://onlinestatbook.com/2/estimation/mean.html))


As we can see 95% of the distribution (shaded blue) is within 1.96 standard deviations of the mean zero on the normal distribution.

See [here](http://onlinestatbook.com/2/estimation/mean.html) for a more detailed explanation of the confidence interval.

But what about the `boot` part of the function?

This is short for the processes of [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)), which means that the mean is calculated many times after taking random samples of the data. This is called [resampling](https://en.wikipedia.org/wiki/Resampling_(statistics)) and in the case of bootstrapping, it is called resampling with replacement because the same values can be reused within even the same sample. For example if our set of possible values is 1,2,3,4,5, we could obtain a sample that is 1,1,3,4,5)).

The mean for say 1000 different bootstrap samples is calculated and plotted as a histogram just like the sampling distribution. 

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "boot.png"))
```
##### [[source]](https://acclab.github.io/bootstrap-confidence-intervals.html)


The confidence interval is then determined from this plot as the point on the where the data is again within a 95% confidence interval around the  most frequently occurring calculated mean across all the different bootstrap samples.

See [here](https://acclab.github.io/bootstrap-confidence-intervals.html) for more details about how this process works.

We can use this function in our plot  as `stat_summary(fun.data = mean_cl_boot)`, we can also add a position argument so that our two groups are not overlapping by choosing the `position_doge` option which takes an argument of `width` which specifies how far apart the two groups should be plotted. 

The confidence intervals are then plotted as a line through a point which shows the range of other possible values for the mean based on the bootstrap samples. 

We will also use the `stat_summary()` function to create a line between the points based on the mean.

```{r}
raw_average <-Annual %>% 
                group_by(year) %>%
                ggplot(aes(x = year, y = Pills_in_millions)) + 
                   stat_summary(fun.data = mean_cl_boot,
                                position = position_dodge(width=0.5)) +
                   stat_summary(fun = mean,
                               geom = "line") +
                   labs(title = "Average Number of Opioid Pills Shipped to a US County",
                            y = "Number of pills in millions") +
                   theme_minimal()

raw_average
```

It looks like the average number of opioid pills shipped to a county peaked in 2011 and has slowly declined. 


We can look a bit deeper if we only calculate a mean for each state. To make a visual of this we will use the `geom_boxjitter()` function of the `ggpol` package.

```{r}
raw_state_avg <- Annual %>% 
     group_by(BUYER_STATE, year) %>%
     summarise( mean_DOSAGE = mean(Pills_in_millions)) %>% ungroup() %>%
     ggplot(aes(x = year, y = (mean_DOSAGE))) + 
        ggpol::geom_boxjitter()+ 
        labs(title = "Average number of opioid pills shipped to a given county for each state",
                 y = "Number of pills in millions")+
        theme_minimal()

raw_state_avg
```

Again we see the same general overall trend, we also see that the spread was quite large with some states receiving many more pills than others. 


## State Shipments over time
To get a better sense of how each state changed over time we can create a line plot instead.


```{r}
Annual  %>% group_by(BUYER_STATE, year) %>%
     summarise( mean_DOSAGE = mean(Pills_in_millions)) %>% ungroup() %>%
ggplot(aes(x = year, 
           y = mean_DOSAGE, 
           group = BUYER_STATE, 
           color = BUYER_STATE)) + 
  geom_line()+
       labs(title = "Average number of opioid pills shipped to a given county for each state",
              y = "Number of pills in millions")+
     theme_minimal()
```

Since we have so many states, the legend is not very useful. Instead we can use the `girafe` package to create an interactive plot that will tell people what state each line represents when they hover over different data points.

```{r}
  
g<-Annual  %>% group_by(BUYER_STATE, year) %>%
     summarise( mean_DOSAGE = mean(Pills_in_millions)) %>% ungroup() %>%
ggplot(aes(x = year, y = mean_DOSAGE, group = BUYER_STATE, color = BUYER_STATE)) +
  geom_line()+
         labs(title = "Average number of opioid pills shipped to a given county for each state",
              y = "Number of pills in millions")+
     theme_minimal()


  g <- g + geom_point_interactive(aes(
                color = BUYER_STATE, 
              tooltip = usdata::abbr2state(BUYER_STATE)), 
                 size = 2,
                alpha = 3/10) +theme(legend.position = "none")
 
girafe(code = print(g))
```

In this plot it appears that the largest number of pills were shipped to counties in California However, since we did not account for population or population density, this could simply be because it is the most populated state. To account for this we will perform something called normalization to make a more fair comparison.


## Normalization of pill count

The term data [normalization](https://en.wikipedia.org/wiki/Normalization_(statistics)) actually has a variety of meanings. 

In some cases it indicates the process of making data "more normally distributed", which means that the data is transformed in a such way that when the frequencies of the various data points are plotted, it resembles that of the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution), which looks like a "bell curve". This may be helpful for performing certain statistical tests that assume that the data is normally distributed.

In other cases, it may mean the process of transforming the data to a common scale so that comparisons can be made fairly. 

In our case we want to compare the number of pills shipped to each county. However, using the raw data results in an unfair comparison as the counties themselves have very different populations. Therefore, if a county has a very large population, we may assume that the large number of pills shipped to that county may indicate that this county received a particularly high amount of opioids, however, it may actually be that this county received far fewer pills per person than a smaller county.

Thus if we divide or scale the number of pills shipped to be relative to the number of people in a given county, then we have the number of pills shipped per person. Thus the data is now on the same scale for each county. 

This can be extended to evaluating differences between states and rural or urban counties by taking the mean of the normalized pill counts per person for each county within each group.

See [here](http://www.pbcgis.com/normalize/#:~:text=To%20normalize%2C%20in%20a%20statistical,over%20unequal%20areas%20or%20populations.) for more information about how this type of normalization is used in [Geographic information system (GIS)](https://en.wikipedia.org/wiki/Geographic_information_system) analyses.

This may be best illustrated with some example data. 

Here we will create a tibble for three imaginary counties. Each has a different population but received the same number of pills. 

Then we will calculate the number of pills per person by dividing the number of pills shipped to that county by the population of that county.

```{r}
example_data <-tibble(population = c(10, 50, 100),
                           pills = c(100, 100, 100))

example_data %<>% mutate(norm_pills = pills/population)

example_data
```


You can see that on average 10 pills were shipped for each person for the first county.

In the second row, the population is much larger, thus despite the same number of pills being shipped to this example county, there were only enough pills shipped for on average 2 per person. In the final row the population is very large, thus only enough pills were shipped to give on average 1 per person.

Note that however, it is likely that only a small portion of the county populations actually received the pills that were shipped to a given county, but this helps us get a sense of the relative amount shipped to each county and likely used by people in the county where the pills were shipped (although this also not certain).


Now we will create a new variable called `pop_DOSAGE` that is the number of pills shipped per county divided by the population of that county:

```{r}

Annual%<>%  mutate(pop_DOSAGE =  DOSAGE_UNIT / population)

glimpse(Annual)
```


Now we will create a plot of the national county average for this normalized pill count over time.


```{r}

norm_average <- Annual %>% 
  group_by(year)  %>%
  ggplot( aes(x = year, y = pop_DOSAGE)) + 
    stat_summary(fun.data = mean_cl_boot,
                 position = position_dodge(width=0.5)) +
    stat_summary(fun = mean,
                geom = "line") +
    labs(title = "Average Number of pills shipped per person for a given county",
             y = "Normalized Number of pills") +
    theme_minimal()

norm_average
```

Now, let's put this with the comparable raw data plot using the `patchwork` package.
Using this package we can use a `+` to combine plots together. We can also continue to use the `theme()` function of the `ggplot2` package to remove the previous titles and then use the `plot_annotation()` function of the `patchwork` package to add an overall title using the `title` argument. This function can also be used with the `theme` argument and then using the `ggplot2` `theme()` function as usual. This will apply changes to all plots that are combined but only regarding to the titles plot margin and background.

```{r}
raw_average +
    theme(axis.text.x = element_text(angle = 90)) +
norm_average + 
  theme(plot.title = element_blank(), axis.text.x = element_text(angle = 90)) +
  patchwork::plot_annotation(title = "Raw vs Normalized Data")
```

We can see that the variability is much lower for the normalized data (however this is in part because the scale is different), but we see the same general trend from one year to the next. 

This is now also a bit easier to interpret. It is easier to think about 30 vs 50 pills per person as opposed to 10 million pills vs 20 million pills for a given county.

Again let's take a deeper look at the states and create a combined box plot and jitter plot figure.

#### {.recall_code_question_block}
<b><u> Question Opportunity </u></b>

Do you recall how to do this? How would you combine the previous plot of the raw data with this plot?

####


<details> <summary> Click here to reveal the code. </summary>


```{r}
norm_state_avg <-Annual  %>% 
  group_by(BUYER_STATE, year) %>%
  summarise( mean_DOSAGE = mean(pop_DOSAGE)) %>% ungroup() %>%
    ggplot(aes(x = year, y = (mean_DOSAGE))) + 
    geom_boxjitter()+ 
    labs(title = "Average number of opioid pills shipped to a given county for each state",
            y = "Number of Pills Per Capita")+
    theme_minimal()

norm_state_avg 


together <- raw_state_avg + 
            theme(axis.text.x = element_text(angle = 90)) +
            norm_state_avg + 
            theme(plot.title = element_blank(), axis.text.x = element_text(angle = 90)) +
            plot_annotation(title = "Raw vs Normalized Data")
```

</details>

```{r}
together

```

We see a similar spread and the same general trend although the difference from one year to the next appears to be steeper for the normalized data.

Now, let's see how normalization changes the state specific data.

#### {.recall_code_question_block}
<b><u> Question Opportunity </u></b>

Do you recall how to create the state specific and interactive plot?

####


<details> <summary> Click here to reveal the code. </summary>


```{r}

g2<-Annual  %>% group_by(BUYER_STATE,year) %>%
     summarise( mean_DOSAGE = mean(pop_DOSAGE)) %>% ungroup() %>%
ggplot(aes(x = year, y = mean_DOSAGE, group = BUYER_STATE, color = BUYER_STATE)) +
  geom_line()


  g2 <- g2 + geom_point_interactive(aes(
                color = BUYER_STATE, 
              tooltip = usdata::abbr2state(BUYER_STATE)), 
                 size = 2,
                alpha = 3/10) +theme(legend.position = "none")
```
 
</details> 

```{r}
girafe(code = print(g2))

```

This dramatically changed the resulting plot!

We can see that now Tennessee, Kentucky, and West Virginia were among the top to receive pills relative to their populations. California is no longer at the top of the plot.



## Rural and Urban Differences

OK, now that we can make fair comparisons between counties, we can now take a look at the differences between rural and urban counties.

To make this plot we will again use the `stat_summary()` function of the `ggplot2` package. 

We want to include both the raw data and the normalized data in the same plot. 


Recall that we can do this using the `facet_wrap()` function. 

<details> <summary> Click here to see how this can be done using `facet_wrap()` </summary>

In this option we use the `labeller` argument of the `facet_wrap()` function to change what the `strip.text` labels will be (which is the text above each plot - the location of which can be changed).

```{r}
Annual %>% pivot_longer(names_to = "type", 
                        values_to  = "value", 
                        cols = c(Pills_in_millions, 
                                 pop_DOSAGE))%>%
  mutate(type = forcats::fct_inorder(type)) %>%

ggplot( aes(y = value, x = year, colour = rural_urban, group = rural_urban)) + 
  stat_summary(fun.data = mean_cl_boot,
               position = position_dodge(width = 0.5),
               geom = "pointrange") +
  stat_summary(fun.y = mean,
                geom = "line") +
  facet_wrap( ~ type, scales = "free", labeller = 
                as_labeller(c(Pills_in_millions = "Raw Data", 
                              pop_DOSAGE = "Normalized Data (pills per capita)"))) +
  labs(title = "Difference in Opioid Shipments With and Without Normalization",
       y = "Number of Pills",
       x = NULL)+
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 90),
        legend.title = element_blank())+
  scale_color_manual(values = c("#20A387FF", "#481567FF"))
```

</details> 
***

Or, instead we can continue to use `patchwork` to combine two different plots. Both options allow for a more flexibility about specifying different aspects of the plot. Notice that in both cases we use a manual color scheme to color the two groups using the `scale_color_manual()` function of the `ggplot2` package. This function takes a `values` argument that must be a list of colors that is equal to the number of groups to be colored. Also the `element_text(hjust = 0.5)` of the `theme()` function, allows for the titles to be centered.

```{r}
Raw_Data <-Annual %>%
ggplot( aes(y = Pills_in_millions, x = year, colour = rural_urban, group = rural_urban)) + 
  stat_summary(fun.data = mean_cl_boot,
               position = position_dodge(width = 0.5),
               geom = "pointrange") +
  stat_summary(fun = mean,
                geom = "line") +
  labs(title = "Raw Data",
       y = "Number of Pills in millions",
       x = NULL)+
  theme_linedraw()+
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        axis.text.x = element_text(angle = 90),
        axis.text = element_text(size = 10),
        legend.title = element_blank(),
        legend.position = "none") +
  scale_color_manual(values = c("#20A387FF", "#481567FF"))


Norm_Data<-Annual %>%
ggplot( aes(y = pop_DOSAGE, x = year, colour = rural_urban, group = rural_urban)) + 
  stat_summary(fun.data = mean_cl_boot,
               position = position_dodge(width = 0.5),
               geom = "pointrange") +
  stat_summary(fun = mean,
                geom = "line") +
  labs(title = "Normalized Data",
       y = "Number of Pills Per Capita",
       x = NULL)+
  theme_linedraw()+
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        axis.text.x = element_text(angle = 90),
        axis.text = element_text(size = 10),
        legend.title = element_blank())+
  scale_color_manual(values = c("#20A387FF", "#481567FF"))

Raw_Data + Norm_Data + plot_annotation(title = "Difference in Opioid Shipments across different types of counties")
```


We can see that without accounting for population the urban counties recieved many more pills than the urban counties. In contrast, when population is taken into account, the rates appear to be very similar.

We can also see that there appears to be much higher variability among the urban counties as compared to the rural counties. 

Let's save our `theme()` function code as an actual theme to be used for our future plots so that they can be stylized similarly.

```{r}
theme_county <-function() {
  theme_linedraw()+
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        axis.text.x = element_text(angle = 90),
        axis.text = element_text(size = 10),
        legend.title = element_blank())
}

```
Now we simply need to type county_theme() instead to achieve the same style for our plot.



## Greater granularity of density

Recall that the [article](https://jamanetwork.com/journals/jamapsychiatry/fullarticle/1874575) that surveyed heroin users in the [Survey of Key Informants’ Patients Program](https://www.radars.org/radars-system-programs/survey-of-key-informants-patients.html) and the [Researchers and Participants Interacting Directly (RAPID) program](https://www.radars.org/radars-system-programs/researchers-and-participants-interacting-directly.html) found that

>A much greater percentage of heroin users completing the survey in the SKIP Program reported currently living in small urban or nonurban areas than in large urban areas (75.2% vs 24.8%) at the time of survey completion. 

This survey used self-declared area of current residence (large urban, small urban, suburban, or rural).

According to the [Organization for Economic Co-operation and Development(OECD)](https://data.oecd.org/popregion/urban-population-by-city-size.htm#:~:text=their%20administrative%20boundaries.-,Urban%20areas%20in%20OECD%20countries%20are%20classified%20as%3A%20large%20metropolitan,areas%20if%20their%20population%20is):

> Urban population by city size is determined by population density and commuting patterns; this better reflects the economic function of cities in addition to their administrative boundaries. Urban areas in OECD countries are classified as: large metropolitan areas if they have a population of 1.5 million or more; metropolitan areas if their population is between 500 000 and 1.5 million; medium-size urban areas if their population is between 200 000 and 500 000; and, small urban areas if their population is between 50 000 and 200 000. This indicator is measured as a percentage of the national population.

Thus the small urban cutoff is populations less than 200,000.

Given that we saw a large degree of variability among the urban counties, we will now use parse this group further to see if examining counties that were either large urban or smaller (included small urban and rural) seems reasonable.

```{r}

Annual %<>% 
  mutate(category = case_when(population >= 200000 ~ "Large Urban",
                              population >= 50000 & 
                              population < 200000 ~ "Small Urban",
                              population < 50000 ~ "Rural"))

```

Let's take a look at this with our formatted table and plots as before.


#### {.think_question_block}
<b><u> Question Opportunity </u></b>

Do you recall how we might do this?

####


<details> <summary> Click here to reveal the code. </summary>


```{r}

R_U <-Annual%>% 
  group_by(year) %>%
  count(category) %>%
  tidyr::pivot_wider(names_from = category,
                     values_from = n) %>%
  ungroup() %>% 
  mutate("Rural Change" = Rural - lag(Rural), 
         "Small Urban Change" = `Small Urban` - lag(`Small Urban`),
         "Large Urban Change" = `Large Urban` - lag(`Large Urban`),
        "Percent Large Urban" = 
  round((`Large Urban`/(`Large Urban` + `Small Urban` + Rural))*100, digits = 2)) %>%
  rename("Year" = "year")

R_U

table_cat <- formattable(R_U, list(`Percent Large Urban` = color_bar("#FA614B"),
                  `Rural Change` = formatter(.tag ="span", 
                                            style = ~style(
                                            color = redgreen(number = 
                                                               `Rural Change`))),
             `Small Urban Change` = formatter(.tag = "span", 
                                            style = ~style(
                                            color = redgreen(number = 
                                                          `Small Urban Change`))),
             `Large Urban Change` = formatter(.tag = "span", 
                                                style = ~style(
                                                color = redgreen(number =
                                                              `Large Urban Change`)))))
```
</details> 

```{r}
table_cat
```

This time when we create our plot we will add labels directly to the lines of our plot using the `directlabels` package. 

To do this we will use the `direct.label()` function. This requires a list of arguments. The `dl.trans()` function modifies the location of all labels, while the `dl.move()` function can move a specific label to a specified location. Method options are shown [here](https://cran.r-project.org/web/packages/directlabels/directlabels.pdf). See [this case study](https://opencasestudies.github.io/ocs-bp-co2-emissions/) and [this case study](https://opencasestudies.github.io/ocs-bp-youth-disconnection/) for more details. 
 

```{r}

plot_cat_raw <-Annual %>%
ggplot( aes(y = Pills_in_millions, x = year, colour = category, group = category)) + 
  stat_summary(fun.data = mean_cl_boot,
             position=position_dodge(width=0.5)) +
  stat_summary(fun = mean,
                geom = "line") +
  labs(title = "Raw Data",
          y = "Pills in millions",
          x = NULL)+
  theme_county()+
  theme(strip.text = element_text(size = 14, face = "bold"))+
  scale_color_manual(values = c("#481567FF", "#20A387FF", "#453781FF"))

 plot_cat_raw<- directlabels::direct.label(plot_cat_raw, 
                                           method = list(dl.trans(y = y +0.5),
                                                     "far.from.others.borders",
                                                     fontface = 'bold'))
  
  plot_cat_norm <-Annual %>%
ggplot( aes(y = pop_DOSAGE, x = year, colour = category, group = category)) + 
  stat_summary(fun.data = mean_cl_boot,
             position=position_dodge(width=0.5)) +
  stat_summary(fun = mean,
                geom = "line") +
  labs(title = "Normalized Data",
          y = "Number of Pills Per Capita",
           x = NULL)+
  theme_county()+
  theme(strip.text = element_text(size = 14, face = "bold"))+
  scale_color_manual(values = c("#481567FF", "#20A387FF", "#453781FF"))

 plot_cat_norm<- direct.label(plot_cat_norm, method = list(dl.trans(y = y +0.5),
                                                     "far.from.others.borders",
                                                     fontface = 'bold'))
 
 plot_cat_raw + plot_cat_norm + plot_annotation(title = "Difference in opioid pill shipments between types of counties",
       subtitle = "Oxycodone and Hydrocodone pills in the US",)
```


Wow, we can see here that the two urban categories actually have larger differences in normalized pill counts from each other than either has with the rural counties!

Thus it seems reasonable to lump the rural and small urban  categories together.

So now we will make a new variable based on being either large urban or small urban/rural:

```{r}

Annual %<>% 
  mutate(large_urban = case_when(population >= 200000 ~ "Large Urban",
                                 population >= 50000 & 
                                 population < 200000 ~ "Small Urban or Rural",
                                 population < 50000 ~ "Small Urban or Rural"))

```

```{r}
plot_2cat_raw <-Annual %>%
ggplot( aes(y = Pills_in_millions, x = year, 
            colour = large_urban, group = large_urban)) + 
  stat_summary(fun.data = mean_cl_boot,
             position=position_dodge(width=0.5)) +
  stat_summary(fun = mean,
              geom = "line") +
  labs(title = "Raw Data",
           y = "Pills in Millions",
           x = NULL)+
  theme_county()+
scale_color_manual(values = c("#481567FF", "#20A387FF"))

plot_2cat_raw <-direct.label(plot_2cat_raw, method = list(dl.trans(y = y +.6),
                                                     "far.from.others.borders",
                                                     fontface = 'bold',
                                                    cex = 0.8))

plot_2cat_norm<-Annual %>%
ggplot( aes(y = pop_DOSAGE, x = year, 
            colour = large_urban, group = large_urban)) + 
  stat_summary(fun.data = mean_cl_boot,
             position=position_dodge(width=0.5)) +
  stat_summary(fun = mean,
              geom = "line") +
  labs(title = "Normalized Data",
           y = "Pills Per Capita",
           x = NULL)+
  theme_county()+
scale_color_manual(values = c("#481567FF", "#20A387FF"))

plot_2cat_norm <-direct.label(plot_2cat_norm, method = list(dl.trans(y = y +.6),
                                                     "far.from.others.borders",
                                                     fontface = 'bold',
                                                      cex = 0.8))


plot_2cat_raw +plot_2cat_norm + plot_annotation(
  title = "Difference in opioid pill shipments between types of counties",
       subtitle = "Oxycodone and Hydrocodone pills in the US")
```

                                                   
Indeed when we evaluate the data in this way, we see that small urban and rural counties received higher numbers of pills per person than large urban counties.

# **Data Analysis**
***

```{r,echo= FALSE, eval = TRUE}
# In case instructors wish to start here, we will load the necessary data

load(file =  here::here("data","Wrangled", "Annual_opioid_data.rda"))
load( file = here::here("data", "Wrangled", "county_info.rda"))

Annual%<>%  mutate(pop_DOSAGE =  DOSAGE_UNIT / population)


Annual %<>% 
  mutate(large_urban = case_when(population >= 200000 ~ "Large Urban",
                                 population >= 50000 & 
                                 population < 200000 ~ "Small Urban or Rural",
                                 population < 50000 ~ "Small Urban or Rural"))

```


## Student t-test

OK, we can tell that there appears to be a difference between small urban and rural counties compared to large urban counties by looking at this plot, however is the difference between these two categories of counties meaningful? To evaluate this we could possibly use a statistical test called the [Student's $t$-test](https://stattrek.com/statistics/dictionary.aspx?definition=two-sample%20t-test), which can be used to determine if [two group means are different](http://onlinestatbook.com/2/estimation/difference_means.html){target="_blank"}.

Let's remind ourselves of one of our original questions, 

> Has there been a difference between opioid pill shipments to rural and urban counties in the US?

In hypothesis testing, we are interested in comparing two different hypotheses: a "null" hypothesis (can be thought of like a baseline e.g. the means between two groups are the same) compared to an "alternative" hypothesis (e.g. the means between two groups are different). We are going to ask if there is enough evidence in our data to reject the null hypothesis.  

Let's try to formalize this a bit. 

Using the student-test,  we can test whether the mean pill number for pills shipped to the rural counties is the same as the mean  number of pill shipped to the urban areas. If we call the true unknown means of the two groups $\mu_U$ and $\mu_R$, for the urban and rural areas, respectively, then we can define the <b>null hypothesis</b> that there is no difference in the two means:

$$ H_0: \mu_U = \mu_R $$  

In contrast, we also define an <b>alternative hypothesis</b> that there is a difference between the mean number of pills shipped to each type of county: 

$$ H_a: \mu_U \neq \mu_R $$

The idea behind a hypothesis test is that we assume the null hypothesis is true and we use our data to help us identify if there is enough evidence to _reject the null hypothesis_. 

This is similar to the idea of assuming that individuals are not guilty until proven otherwise.
If there is not enough evidence in the data, then we say we "fail to reject the null hypothesis".


However, performing this test depends on certain assumptions about our data:

1) The data for each group is [normally distributed](http://onlinestatbook.com/2/introduction/distributions.html){target="_blank"}.
2) The [variance](https://stattrek.com/statistics/dictionary.aspx?definition=variance){target="_blank"} of both groups is similar.
3) The observations from the two groups are [independent](https://www.stat.cmu.edu/~cshalizi/36-220/lecture-5.pdf){target="_blank"} (meaning that observations do not influence each other).
4) The observations within each group are [independent](https://www.stat.cmu.edu/~cshalizi/36-220/lecture-5.pdf){target="_blank"} (meaning that observations do not influence each other).


OK, for the first assumption, we can test if each group to be tested in normally distributed by making what is called a "quantile-quantile" plot (or <b>Q-Q plot</b> for short). When we talk about a <b>quantile</b>, we are talking about dividing up the distribution of the data into roughly equal portions where roughly the same number of observations fall into each portion. For example, if you divide your data into 100 quantiles, you can think about this as percentiles, but you could also divide your data into 10 quantiles and these would be called deciles. 

<b>Why Q-Q plots?</b> This plot allows us to compare the quantiles of two distributions together: (1) quantiles of a known theoretical distribution (like the normal distribution) compared to (2) quantiles of the distribution of our data.  If the quantiles from these two distributions line up in the plot, then that is a visual piece of evidence that our data follow that theoretical distribution (like the normal distribution).  

<b>How does this work?</b> To do this we will plot the quantiles of our data on the y-axis and the quantiles of the theoretical normal distribution on the x-axis. If the quantiles line up then we can say that our data is fairly normal. See [here](http://onlinestatbook.com/2/advanced_graphs/q-q_plots.html) for more information about Q-Q plots.

Using the `stat_qq()` function of the `ggplot2` package, we can easily create a Q-Q plot for our data randomly sampled from a normal distribution ("sample") and compare it to the quantiles from a normal distribution ("theoretical").  The default comparison distribution for these functions is the normal distribution, so we don't need to specify it in our code.

```{r}
Annual %>% pivot_longer(names_to = "type", values_to = "value", cols = c(pop_DOSAGE, DOSAGE_UNIT)) %>%
  ggplot(aes(sample = value)) +
  stat_qq() + 
  stat_qq_line() +
  facet_wrap(~large_urban + type, scales = "free")

Annual %>% pivot_longer(names_to = "type", values_to = "value", cols = c(pop_DOSAGE, DOSAGE_UNIT)) %>%
  ggplot(aes(sample = value)) +
  stat_qq() + 
  stat_qq_line() +
  facet_wrap(~rural_urban + type, scales = "free")
```

The `stat_qq_line()` function is used to add a line (computes the slope an intercept) on the plot. If the points lie on this straight line, this is evidence that the data have a normal distribution, not that the data have a particular normal distribution.

OK, we can see that in all cases the points appear too deviate from the line for at least one group, indicating that the quantiles are fairly dissimilar between the observed and theoretical data.

We can see if we can overcome this by transforming the data by log scaling the number of pills or the normalized number of pills. 

```{r}

Annual %>% pivot_longer(names_to = "type", values_to = "value", cols = c(pop_DOSAGE, DOSAGE_UNIT)) %>%
  ggplot(aes(sample = log(value))) +
  stat_qq() + 
  stat_qq_line() +
  facet_wrap(~large_urban + type, scales = "free")

Annual %>% pivot_longer(names_to = "type", values_to = "value", cols = c(pop_DOSAGE, DOSAGE_UNIT)) %>%
  ggplot(aes(sample = log(value))) +
  stat_qq() + 
  stat_qq_line() +
  facet_wrap(~rural_urban + type, scales = "free")
  
```

OK, this did not help very much. So now we have two options, we can continue with the [Student's $t$-test](https://stattrek.com/statistics/dictionary.aspx?definition=two-sample%20t-test) because this test is fairly **robust** to violations of the normality assumption if the sample size is relatively large, due to what is called the [central limit theorem](https://www.analyticsvidhya.com/blog/2019/05/statistics-101-introduction-central-limit-theorem/){target="_blank"}, which states that as samples get larger, the sample mean has an approximate normal distribution.  
 
```{r}
Annual %>%
  count(large_urban)

Annual %>%
   count(rural_urban)
```

Our samples are indeed quite large, thus it would probably be reasonable to continue with the $t$-test. However, we can also perform a test called the 
[Mann Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) also known as the Wilcoxon rank sum test or the two-sample Wilcox test or the Mann–Whitney–Wilcoxon (MWW) test. Importantly, this test does not rely on the assumption that the data is normally distributed and it is more robust than the t-test to [outliers](https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm) (extreme values compared to the others). Given that the data appears to be quite different from the normal distribution, we will proceed using this test. 

## Mann–Whitney–Wilcoxon (MWW) test

In this test, the hypothesis is slightly different. Instead of comparing means,  this test evaluates the following null hypothesis according to Wikipedia:

> that the probability that randomly selected values of X (1st group) are greater than randomly selected values of Y (2nd group) is equal to the probability that randomly selected values of Y (2nd group) are greater than randomly selected values of X (1st group).

The alternative hypothesis would then be that this probability is not equal. 

We can also use a one-sided alternative to test that random values of X are greater than random values of Y or that  random values of X are less than random values Y. 

Another way of describing this is that the distributions of the probabilities of the occurrences of the range of possible values of X and Y are equal or have a shift of mu = zero. Where as, the alternative would be that the shift between the probability distributions is not zero. and that the one-sided alternatives are either the shift being greater than zero or less than zero. 

Here you can see an illustration of the null hypothesis on the left and a one-sided hypothesis on the right:

```{r}
include_graphics(here::here("img", "null.png"))
```

##### [[source]](https://www.stat.auckland.ac.nz/~wild/ChanceEnc/Ch10.wilcoxon.pdf)


So in our case using the Wilcox test,  we can test whether the null hypothesis that there is an equal probability that random subsets of counties categorized as rural counties is larger than the number of pills shipped to a random subset of counties categorized as urban (or to compare small urban and rural counties vs large urban counties). 

If we call the random subsets of counties $U$ and $R$, for the urban and rural areas, respectively, then we can define the <b>null hypothesis</b> that there is no difference in the probabilities $P$:

$$ H_0: P(U > R) = P(R>U) $$  

In contrast, we also define an <b>alternative hypothesis</b> that there is a difference (two-sided): 

$$ H_a: P(U > R) \neq P(R>U)$$

With a one-sided alternative that a larger number of pills are shipped per person to Rural counties as:

$$ H_a: P(R>U) > P(U > R) $$
Here the probability that a random subset of rural counties had a larger number of pills shipped than that of urban counties is greater than the probabilities of a random subset of urban counties having a larger number of pills than a subset of rural counties. 

We can extend this to the small urban/rural vs the large rural county comparison.


To implement this test in R we will use the `wilcox.test()` in  the `stats` package. X is automatically the group that comes first alphabetically, while Y is the group that comes second alphabetically. 

Thus for the `rural_urban` variable, the `**R**ural` values would be the X group, while the `**U**rban` would be the Y group, as R comes before U in the alphabet.

For the `large_urban` variable, the `**L**arge Urban` group values would be the X group, while the `**S**mall Urban or Rural` values would be the Y group.


This test statistic $W$ is actually quite simple to perform manually with small sample sizes. 

***

<details> <summary> Click here to see how $W$ is calculated manually </summary>

Let's say that the US only had 3 counties that were Rural and 3 counties that were Urban and we wanted to compare the distributions using this test.


The number of pills shipped to each of the 3 Rural counties was:   
1) 10   
2) 50  
3) 30  

The number of pills shipped to each of the 3 urban counties was:  
1) 20  
2) 25  
3) 12  


The first step would be to list out the counties by order of the number of pills shipped. We will use  "R" or "C" if the number came from a rural or urban county:   

County: R   U   U    U   R    R   
 pills: 10  12  20   25  30   40   
  rank: 1   2   3    4   5    6  
  
  
sum of ranks $(R_1)$ for R: $1+ 5 +6 = 12$  
sum of ranks $(R_2)$ for U: $2+3+4 = 9$  
  

Now two $W$ statistics are calculated, one for each group like so (where $n$ is the sample size for that group):  

  $$W_1 = R_1-\frac{n_1(n_1+1)}{2}$$  

  $$W_2 = R_2-\frac{n_2(n_2+1)}{2}$$  
  
   
Using our example data:  

  $W_1$ is for rural counties  
  $W_1 = R_1-\frac{n_1(n_1+1)}{2}$  
  $W_1 = 12 -(3(4)/2) = 12-6 = 6$  
  
  $W_2$ = U for urban counties  
  $W_2 = R_2-\frac{n_2(n_2+1)}{2}$  
  $W_2 = 9-(3(4)/2) = 9-6 = 3$  
  
  Many definitions of $W$ are as follows:  

  $W = min(W_1, W_2) = 3$. However, R calculates $W$  based on what sample is listed first (or X in or description of the hypothesis test). Thus the output will give the $W_x$ for the first group. 
  
  In our case the first group (rural counties) had a $W$ of 6.  

From the documentation for the `stats` package for the `wilcox.text()` function:  
  
> R's value can also be computed as the number of all pairs (x[i], y[j]) for which y[j] is not greater than x[i], the most common definition of the Mann-Whitney test.  

If you are familiar with linear algebra, this can also be calculated by getting all the pairs of values between the two groups using the `outer()` base function and using the `>` (instead of the product or `*` function as this is typically used by default to get the [outer product](https://en.wikipedia.org/wiki/Outer_product)) to determine how often the rural counties have a greater value than the urban counties.

```{r}
example <-tibble(pills = c(10,12, 20, 25,30, 40), county = c("R", "U", "U", "U", "R", "R"))
example

R = c(10,30, 40)
U = c(12, 20, 25)

outer(R,U, ">")

# 10 is less than 12, 20, or 25 (R is always less than U for all 3 comparisons)
# 30 is greater than 12, 20, or 25 (3 times that R is greater than U)
# 40 is greather than 12, 20, or 25 ( 3 tiems that R is greater than U)

# 3+3 = 6 pairs out of 9 where R was greater than U thus W is equal to 6

sum(outer(R,U, ">"))
wilcox.test(data = example,  paired = FALSE,  alternative = "greater", pills~ county)

```


We can see that if we switched the order of our counties, (thus we make the values that were for urban now the values for rural) we then get the $W$ for what was previously our second group in the result. (Remember what ever is alphabetically first will be the first group - so again the results are for "R")

```{r }

example <-tibble(pills = c(10,12, 20, 25,30, 40), county = c("U", "R", "R", "R", "U", "U"))
example

wilcox.test(data = example,  paired = FALSE,  alternative = "greater", pills~ county)

```

for small samples, this can then be compared to a [critical $W$ table](https://math.usask.ca/~laverty/S245/Tables/wmw.pdf) (note here that $W$ is $U$) for comparison to determine significance. If the $W$ is lower than the critical value, it suggests that the null hypothesis should be rejected. From this table we see that our example has so few values that the null can't be reliably rejected. Although the small number was useful for illustrative purposes as to how the test is performed.

For larger samples (like our actual data - generally n>20 per group) the $W$ statistic is instead used to calculate a $Z$ statistic like so:

$$Z = \frac{W-\mu_W}{\sigma_W}$$

Where $\mu_W$ is the expected $W$ if the two groups have identical distributions and $\sigma_W$ is the standard deviation. 

They are calculated as follows:

$$\mu_W = \frac{n_1n_2}{2}$$

$$\sigma_W = \sqrt{\frac{n_1n_2(n_1+n_2+1)}{12}}$$
This can then be used in a [$Z$ table](https://www.simplypsychology.org/z-table.html#:~:text=A%20z%2Dtable%2C%20also%20called,standard%20normal%20distribution%20(SND).)  or using a [calculator](https://www.socscistatistics.com/pvalues/normaldistribution.aspx) to determine a $p$-value.

Here is also a [link](https://youtu.be/BT1FKd1Qzjw) to a video for a more detailed explanation about calculating this by hand.

</details>
***

Now, to implement this test in R we can use the `wilcox.test()` of the `stats` package. 

We need to specify that we want to look for differences of normalized pill counts (`pop_DOSAGE`) between the `rural_urban` groups. We can do so by using the `~` to indicate the dependent variable on the left and the independent variable on the right. We assume that the number of normalized pills depends on the county group, thus the normalized pill count is the dependent variable. Our data is not paired, thus we use the `paried = FALSE` argument. This would be the case if we had comparable rural and urban counties for every state, but that is not the case. See [this case study](https://opencasestudies.github.io/ocs-bp-rural-and-urban-obesity/) for an example of the paired version of this test. We will use the` alternative =` argument to specify that we expect the first of the groups by alphabet which would  would be `Rural` of the `rural_urban` variable to have greater values than the `Urban` group with the `greater` argument. Thus we will also use the `less` argument for the `large_urban` group comparison as we expect the first group (alphabetically) `Large Urban`to have smaller values than the `Small Urban or Rural` group.

```{r}

wilcox.test(pop_DOSAGE ~ rural_urban, data = Annual,
                                    paired = FALSE, 
                               alternative = "greater", 
                                  conf.int = TRUE,
                                  estimate = TRUE)


wilcox.test(pop_DOSAGE ~ large_urban, data = Annual, 
                                    paired = FALSE, 
                               alternative = "less", 
                                  conf.int = TRUE,
                                  estimate = TRUE)
```

Both results have p values that are less than 0.05, which is the threshold commonly used in hypothesis testing to determine if there is enough evidence to reject the null hypothesis. 

In both cases this indicates that indeed there is enough evidence, and we can reject the null hypothesis. Furthermore, the confidence interval does not overlap zero for either test, suggesting that the `Rural` group is larger for the first test and that the `Large Rural` group is smaller for the second test.

For the second test, the absolute value of the difference estimate is larger than that of the first test suggesting that there is a larger difference between these groups than the `Rural` and `Urban` groups.

What about if we had not normalized the data, what would our results be like?

```{r}

wilcox.test(DOSAGE_UNIT ~ rural_urban, data = Annual,
                                    paired = FALSE, 
                               alternative = "greater", 
                                  conf.int = TRUE,
                                  estimate = TRUE)


wilcox.test(DOSAGE_UNIT ~ large_urban, data = Annual, 
                                    paired = FALSE, 
                               alternative = "less", 
                                  conf.int = TRUE,
                                  estimate = TRUE)


```

Using the raw data, we see that there is no difference between the rural and urban categories, and the rural data actually shows lower values than the urban (based on the sign of the estimated difference). Similarly, there was no difference between small urban and rural counties vs. large urban counties, in this case the large urban counties (the first group by alphabetically order) also had larger values (based on the sign of the estimated difference).

Notice that in both of these tests the Confidence Interval overlaps zero. While this is not the case for the other previous tests. When the confidence interval overlaps zero that indicates that there is the possibility that the two populations have in fact no difference.

The main conclusion here is that we can see very different results depending on how we define the data and how we normalize the data!


# **Summary**
*** 

## Summary Plot

Now, let's create a plot that summarizes our main findings.


We will create a plot that is just a single arrow using `geom_segment()` function of `ggplot2`.  


This requires the following arguments :
1) x -  x coordinate of starting point of the arrow
2) y - y coordinate of starting point of the arrow
3) xend -  x coordinate of end point of the arrow
4) yend - y coordinate of end point if the arrow

(Note: You can make a straight arrow using `geom_curve()` with the same arguments)

Also `ggplot` requires a data input, we will use this to create the y axis value for the arrow. We will use the `aes()` function to define the length and location of the arrow.


We need to use the `arrow` argument with the `arrow()` function to create an arrow. The `lineend` and `linejoin` arguments specify what styles to use. The `size()` function specifies how thick the arrow is displayed.

See [here](https://ggplot2.tidyverse.org/reference/geom_segment.html) for different arrow style options.

Then we will use the `xlim()` function of the `ggplot2` package to specify the overall size of the plot relative to the arrow.



```{r}
arrow_df <- data.frame( y = 1)
arrow_df

arrowplot<-ggplot(arrow_df, aes(x = .5, y = 1, xend = .7, yend = 1)) +
                geom_segment(arrow = arrow(),
                           lineend = "butt",
                          linejoin = "mitre",
                              size = 3)+
                xlim(0.5, 0.75)

arrowplot
```

Then we will use `theme_void()` remove  the background.


```{r}
arrowplot <- arrowplot +  theme_void()
arrowplot
```

We can move the legend to the plot area using the `legend.justification` and `legend.position` arguments of the `theme()` function. The relative position on the plot area needs to be specified like so `c(x,y)`. Thus `c(0.7,0.2)` is about 70% across the x axis from right to left and about 20% up from the x axis. 



```{r}

main_plot <- Raw_Data  + 
  theme(legend.justification = c(0.7,0.2),
             legend.position = c(0.7,.2)) +
             arrowplot +
             Norm_Data +
  theme(legend.justification = c(0.7,0.1),
             legend.position = c(0.7,.1)) +
             arrowplot +
             plot_cat_norm + 
  labs(title = "Normalized &\nStratified Data") + 
  plot_layout( nrow = 1, widths = c(5,1.3,5,1.3,5))+
 #theme(title = element_text(hjust = 0.5)) +
  plot_annotation(title = "How we normalize pill shipment data and how we stratify by different types of\ncounties can yield very different pictures about opioid (oxycodone and \nhydrocodone) pill shipments to counties in the US") &
  theme(plot.title = element_text(size = 18, face = "bold", hjust = 0.5))

```


```{r, echo= FALSE}
png(filename = here::here("img", "mainplot.png"), 
         res = 300, width = 10, height = 6, units = "in")
main_plot
dev.off()
```

```{r, echo=FALSE}
knitr::include_graphics(here("img",
                             "mainplot.png"))
```

## Synopsis

In this case study we evaluated the number of [oxycodone](https://en.wikipedia.org/wiki/Opioid_epidemic_in_the_United_States) and [hydrocodone](https://en.wikipedia.org/wiki/Opioid_epidemic_in_the_United_States) pills shipped to pharmacies and practitioners at the county-level around the United States (US) from 2006 to 2014. This data comes form the [Automated Reports and Consolidated Ordering System (ARCOS)](https://www.deadiversion.usdoj.gov/arcos/retail_drug_summary/) of the [DEA](https://www.dea.gov/) and was released by the [Washington Post](https://www.washingtonpost.com/). We compared counties based on the level of urbanicity based on the fact that previous research suggests that regions of the country that are more rural may have communities that were more vulnerable to opioid abuse than that of large urban areas due to likely higher rates of prescription for opioid pills in the time period that we evaluated. 

Our results also suggests that this is the case, as we identified higher numbers of opioid pill shipments to counties that were more rural and less urban.However we learned that how regions are defined and how the data is normalized can have great impact on the results of such comparisons. Furthermore, other factors may be involved in creating a situation that may leave members of communities more vulnerable to drug abuse and addiction, such as socioeconomic factors. Further research is necessary to better understand why the opioid crisis became such a problem in the US and particularly to better understand what current populations are particularly vulnerable and why. 


# **Suggested Homework**
*** 

Students could focus on the counties of a particular state and perform the same analyses and visualizations to see how the different types of counties compared for opioid pill shipments. Students could be asked to work on different states. Discussion could follow about  how and why the states show different results.

# **Additional Information**
***
Note: Monthly data is also available within the `data` directory. This could be used for time series analysis. 

## Helpful Links

[RStudio](https://rstudio.com/products/rstudio/features/){target="_blank"}  
[Cheatsheet on RStuido IDE](https://github.com/rstudio/cheatsheets/raw/master/rstudio-ide.pdf){target="_blank"}  
[Other RStudio cheatsheets](https://rstudio.com/resources/cheatsheets/){target="_blank"}   
[RStudio projects](https://r4ds.had.co.nz/workflow-projects.html)

[Tidyverse](https://www.tidyverse.org/){target="_blank"}   

   
[Piping in R](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html){target="_blank"}   

[application prgoramming interface (API)](https://en.wikipedia.org/wiki/API)  
[JavaScript Object Notation (JSON)](https://fileinfo.com/extension/json#:~:text=A%20JSON%20file%20is%20a,web%20application%20and%20a%20server.)  
[Lightweight programming languagnes](https://en.wikipedia.org/wiki/Lightweight_programming_language)   

[Table formats](https://en.wikipedia.org/wiki/Wide_and_narrow_data){target="_blank"}

[`ggplot2` package](http://ggplot2.tidyverse.org){target="_blank"}    
Please see [this case study](https://opencasestudies.github.io/ocs-bp-co2-emissions/)  for more details on using `ggplot2`    
[grammar of graphics](http://vita.had.co.nz/papers/layered-grammar.html){target="_blank"}   
[`ggplot2` themes](https://ggplot2.tidyverse.org/reference/ggtheme.html){target="_blank"}   
[Normalization](https://en.wikipedia.org/wiki/Normalization_(statistics))  
[Mann–Whitney–Wilcoxon (MWW) test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) also known as the Wilcoxon rank sum test or the two-sample Wilcox test   

Also see [here](https://www.stat.auckland.ac.nz/~wild/ChanceEnc/Ch10.wilcoxon.pdf) for more information about this test and [here](https://youtu.be/BT1FKd1Qzjw) for a video for a more detailed explanation about performing this test by hand.


[Outliers](https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm)  
[Normal distribution](http://onlinestatbook.com/2/introduction/distributions.html)
[Q-Q plots](http://onlinestatbook.com/2/advanced_graphs/q-q_plots.html) 
[Student's $t$-test](https://stattrek.com/statistics/dictionary.aspx?definition=two-sample%20t-test)  
[Confidence interval](http://onlinestatbook.com/2/estimation/mean.html)   
[Sampling distribution](https://en.wikipedia.org/wiki/Sampling_distribution)   
[Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))    [Resampling](https://en.wikipedia.org/wiki/Resampling_(statistics))  

[Motivating report for this case study](https://www.cdc.gov/mmwr/volumes/68/wr/pdfs/mm6802a1-H.pdf)  

Also see this [article](https://jamanetwork.com/journals/jamapsychiatry/fullarticle/1874575) which surveyed heroin users in the [Survey of Key Informants’ Patients Program](https://www.radars.org/radars-system-programs/survey-of-key-informants-patients.html) and the [Researchers and Participants Interacting Directly (RAPID) program](https://www.radars.org/radars-system-programs/researchers-and-participants-interacting-directly.html).  


The data for this case study is available at this [API](https://arcos-api.ext.nile.works/__swagger__/). 

This data is from the [DEA](https://www.dea.gov/) [Automated Reports and Consolidated Ordering System (ARCOS)](https://www.deadiversion.usdoj.gov/arcos/retail_drug_summary/) and was released by the [Washington Post](https://www.washingtonpost.com/).   

A wrapper package about this API is available [here](https://github.com/wpinvestigative/arcos).

 <u>**Packages used in this case study:** </u>

Package   | Use in this case study                                                                      
---------- |-------------
[readxl](https://readxl.tidyverse.org/index.html) | to import an excel file   
[httr](https://httr.r-lib.org/) | to retrieve data from an API   
[tibble](https://tibble.tidyverse.org/) | to create tibbles (the tidyverse version of dataframes)   
[jsonlite](https://cran.r-project.org/web/packages/jsonlite/jsonlite.pdf) | to parse json files   
[stringr](https://stringr.tidyverse.org/){target="_blank"}      | to manipulate  character strings within the data (subset and detect parts of strings)    
[dplyr](https://dplyr.tidyverse.org/){target="_blank"}      | to filter, subset, join, and modify and summarize the data   
[magrittr](https://magrittr.tidyverse.org/){target="_blank"}      | to pipe sequential commands   
[tidyr](https://tidyr.tidyverse.org/){target="_blank"}      | to change the shape or format of tibbles to wide and long   
[naniar](https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html) | to get a sense of missing data   
[ggplot2](https://ggplot2.tidyverse.org/){target="_blank"}      | to create plots
[formattable](https://cran.r-project.org/web/packages/formattable/formattable.pdf) | to create a formatted table
[forcats](https://forcats.tidyverse.org/){target="_blank"}      | to reorder factor for plot
[ggpol](https://cran.r-project.org/web/packages/ggpol/ggpol.pdf) | to create plots that are have jitter and half boxplots   
[ggiraph](https://cran.r-project.org/web/packages/ggiraph/ggiraph.pdf)   | to create interactive plots
[patchwork](https://cran.r-project.org/web/packages/patchwork/patchwork.pdf) | to combine plots
[directlabels](https://cran.r-project.org/web/packages/directlabels/directlabels.pdf) | to add labels directly on lines within plots

#### {.emphasis_block}

If you or a loved one is struggling with opioid addiction, contact the SAMHSA’s National Helpline at [1-800-662-HELP (4357)](tel:1-800-662-HELP (4357)). 

It is a free, confidential, 24/7, 365-day-a-year treatment referral and information service (in English and Spanish) for individuals and families facing mental and/or substance use disorders.

You can also contact the [Addiction Center](https://www.addictioncenter.com/drugs/overdose/) at [(877)871-3575](tel:877871-3575) which also has a confidential 24/7 live chat at:
[https://www.addictioncenter.com/drugs/overdose/](https://www.addictioncenter.com/drugs/overdose/).

According to their website:

>Remember, that being able to treat an overdose at home is not a replacement for a hospital. Even if the moment has passed, and the victim seems fine, there is still a chance that something is going on that cannot be seen by the human eye. Taking the victim to the hospital, can mean the difference between life and death.

> Overdose is a scary word. We often associate it with death, but the two are not always connected. Life can go on after an overdose, but only if the person suffering understands and learns from it. Getting on the road to recovery is not easily done but it is always possible, and the only guaranteed way to never suffer an overdose again. If you don’t know where this path begins, or need help getting help for a loved one, please reach out to a dedicated treatment provider. They’re here, 24/7, to answer any questions you may have. Be it for yourself or someone else.

According to [harmreduction.org](https://harmreduction.org/issues/overdose-prevention/overview/overdose-basics/recognizing-opioid-overdose/), the following are signs of an overdose:

- Loss of consciousness
-Unresponsive to outside stimulus
- Awake, but unable to talk
- Breathing is very slow and shallow, erratic, or has stopped
- For lighter skinned people, the skin tone turns bluish purple, for darker skinned people, it turns grayish or ashen.
- Choking sounds, or a snore-like gurgling noise (sometimes called the “death rattle”)
- Vomiting
- Body is very limp
- Face is very pale or clammy
- Fingernails and lips turn blue or purplish black
- Pulse (heartbeat) is slow, erratic, or not there at all
 
If someone is making unfamiliar sounds while “sleeping” it is worth trying to wake him or her up. Many loved ones of users think a person was snoring, when in fact the person was overdosing. These situations are a missed opportunity to intervene and save a life.

Sometimes it can be difficult to tell if a person is just very high, or experiencing an overdose.  If you’re having a hard time telling the difference, it is best to treat the situation like an overdose – it could save someone’s life.

**The most important thing is to act right away!**

```{r, echo = FALSE}
knitr::include_graphics("https://miro.medium.com/max/700/1*CdiSAr3OomVFHC6_1E_XKw.png")
```

##### [[source]](https://medium.com/dr-ming-kao/opioid-adverse-effects-alternatives-3fae66b7d247)

####

## Session Info

```{r}
sessionInfo()
```


## Acknowledgements

We would like to acknowledge [Brendan Saloner](https://www.jhsph.edu/faculty/directory/profile/2929/brendan-saloner) for assisting in framing the major direction of the case study.

We would also like to acknowledge the [Bloomberg American Health Initiative](https://americanhealth.jhu.edu/) for funding this work. 

